{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Nh6TvNftHj"
      },
      "source": [
        "# Video Processing\n",
        "\n",
        "Converting the video into frames for processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCkgfsgcrQ8D",
        "outputId": "b96c21ea-594d-4963-d278-adab88bfdb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Keys in HDF5: ['camera', 'confidences', 'transforms']\n",
            "\n",
            "--- camera ---\n",
            "Sub-keys: ['intrinsic']\n",
            "\n",
            "--- confidences ---\n",
            "Sub-keys: ['hip', 'leftArm', 'leftForearm', 'leftHand', 'leftIndexFingerIntermediateBase', 'leftIndexFingerIntermediateTip', 'leftIndexFingerKnuckle', 'leftIndexFingerMetacarpal', 'leftIndexFingerTip', 'leftLittleFingerIntermediateBase', 'leftLittleFingerIntermediateTip', 'leftLittleFingerKnuckle', 'leftLittleFingerMetacarpal', 'leftLittleFingerTip', 'leftMiddleFingerIntermediateBase', 'leftMiddleFingerIntermediateTip', 'leftMiddleFingerKnuckle', 'leftMiddleFingerMetacarpal', 'leftMiddleFingerTip', 'leftRingFingerIntermediateBase', 'leftRingFingerIntermediateTip', 'leftRingFingerKnuckle', 'leftRingFingerMetacarpal', 'leftRingFingerTip', 'leftShoulder', 'leftThumbIntermediateBase', 'leftThumbIntermediateTip', 'leftThumbKnuckle', 'leftThumbTip', 'neck1', 'neck2', 'neck3', 'neck4', 'rightArm', 'rightForearm', 'rightHand', 'rightIndexFingerIntermediateBase', 'rightIndexFingerIntermediateTip', 'rightIndexFingerKnuckle', 'rightIndexFingerMetacarpal', 'rightIndexFingerTip', 'rightLittleFingerIntermediateBase', 'rightLittleFingerIntermediateTip', 'rightLittleFingerKnuckle', 'rightLittleFingerMetacarpal', 'rightLittleFingerTip', 'rightMiddleFingerIntermediateBase', 'rightMiddleFingerIntermediateTip', 'rightMiddleFingerKnuckle', 'rightMiddleFingerMetacarpal', 'rightMiddleFingerTip', 'rightRingFingerIntermediateBase', 'rightRingFingerIntermediateTip', 'rightRingFingerKnuckle', 'rightRingFingerMetacarpal', 'rightRingFingerTip', 'rightShoulder', 'rightThumbIntermediateBase', 'rightThumbIntermediateTip', 'rightThumbKnuckle', 'rightThumbTip', 'spine1', 'spine2', 'spine3', 'spine4', 'spine5', 'spine6', 'spine7']\n",
            "\n",
            "--- transforms ---\n",
            "Sub-keys: ['camera', 'hip', 'leftArm', 'leftForearm', 'leftHand', 'leftIndexFingerIntermediateBase', 'leftIndexFingerIntermediateTip', 'leftIndexFingerKnuckle', 'leftIndexFingerMetacarpal', 'leftIndexFingerTip', 'leftLittleFingerIntermediateBase', 'leftLittleFingerIntermediateTip', 'leftLittleFingerKnuckle', 'leftLittleFingerMetacarpal', 'leftLittleFingerTip', 'leftMiddleFingerIntermediateBase', 'leftMiddleFingerIntermediateTip', 'leftMiddleFingerKnuckle', 'leftMiddleFingerMetacarpal', 'leftMiddleFingerTip', 'leftRingFingerIntermediateBase', 'leftRingFingerIntermediateTip', 'leftRingFingerKnuckle', 'leftRingFingerMetacarpal', 'leftRingFingerTip', 'leftShoulder', 'leftThumbIntermediateBase', 'leftThumbIntermediateTip', 'leftThumbKnuckle', 'leftThumbTip', 'neck1', 'neck2', 'neck3', 'neck4', 'rightArm', 'rightForearm', 'rightHand', 'rightIndexFingerIntermediateBase', 'rightIndexFingerIntermediateTip', 'rightIndexFingerKnuckle', 'rightIndexFingerMetacarpal', 'rightIndexFingerTip', 'rightLittleFingerIntermediateBase', 'rightLittleFingerIntermediateTip', 'rightLittleFingerKnuckle', 'rightLittleFingerMetacarpal', 'rightLittleFingerTip', 'rightMiddleFingerIntermediateBase', 'rightMiddleFingerIntermediateTip', 'rightMiddleFingerKnuckle', 'rightMiddleFingerMetacarpal', 'rightMiddleFingerTip', 'rightRingFingerIntermediateBase', 'rightRingFingerIntermediateTip', 'rightRingFingerKnuckle', 'rightRingFingerMetacarpal', 'rightRingFingerTip', 'rightShoulder', 'rightThumbIntermediateBase', 'rightThumbIntermediateTip', 'rightThumbKnuckle', 'rightThumbTip', 'spine1', 'spine2', 'spine3', 'spine4', 'spine5', 'spine6', 'spine7']\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "import h5py\n",
        "\n",
        "with h5py.File('0.hdf5', 'r') as f:\n",
        "    print(\"Keys in HDF5:\", list(f.keys()))\n",
        "\n",
        "with h5py.File('0.hdf5', 'r') as f:\n",
        "    for key in ['camera', 'confidences', 'transforms']:\n",
        "        print(f\"\\n--- {key} ---\")\n",
        "        if isinstance(f[key], h5py.Group):\n",
        "            print(f\"Sub-keys: {list(f[key].keys())}\")\n",
        "        else:\n",
        "            print(f\"Dataset shape: {f[key].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIaBPhgOreB_",
        "outputId": "98e53976-cac0-453e-a3ec-5ba4833d2566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intrinsics extracted. Shape: (3, 3)\n",
            "Camera trajectory extracted. Shape: (94, 4, 4)\n"
          ]
        }
      ],
      "source": [
        "!mkdir ego_dex_data\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File('0.hdf5', 'r') as f:\n",
        "    intrinsics = f['camera']['intrinsic'][:]\n",
        "    print(\"Intrinsics extracted. Shape:\", intrinsics.shape)\n",
        "    trajectory = f['transforms']['camera'][:]\n",
        "    print(\"Camera trajectory extracted. Shape:\", trajectory.shape)\n",
        "\n",
        "    np.save(\"ego_dex_data/intrinsics.npy\", intrinsics)\n",
        "    np.save(\"ego_dex_data/trajectory.npy\", trajectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWPnT_IYr23K",
        "outputId": "418b699c-f67c-4eee-fb66-c6afee7415c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 0 frames to ego_dex_data/rgb/\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "video_path = '0.mp4'\n",
        "output_folder = 'ego_dex_data/rgb/'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "count = 0\n",
        "\n",
        "while count < 94:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    file_path = os.path.join(output_folder, f\"{count:04d}.jpg\")\n",
        "    cv2.imwrite(file_path, frame)\n",
        "    count += 1\n",
        "\n",
        "cap.release()\n",
        "print(f\"Extracted {count} frames to {output_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PC1j6lrrL9Y"
      },
      "source": [
        "# Pose Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqCRu2KKtww-",
        "outputId": "2474af40-e961-4658-eba4-2ddce041db33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Drawing Skeleton...\n",
            "Skeleton video saved to 0_pose_estimation.mp4\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "import h5py\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "H5_FILE = '0.hdf5'\n",
        "VIDEO_FILE = '0.mp4'\n",
        "OUTPUT_FILE = '0_pose_estimation.mp4'\n",
        "\n",
        "FINGER_NAMES = ['Index', 'Middle', 'Ring', 'Little']\n",
        "SKELETON_MAP = []\n",
        "\n",
        "def build_hand_map(side):\n",
        "    \"\"\"Generates the connectivity list for one hand (left or right)\"\"\"\n",
        "    hand_root = f'{side}Hand'\n",
        "    connections = []\n",
        "\n",
        "    connections.append((hand_root, f'{side}ThumbKnuckle'))\n",
        "    connections.append((f'{side}ThumbKnuckle', f'{side}ThumbIntermediateBase'))\n",
        "    connections.append((f'{side}ThumbIntermediateBase', f'{side}ThumbIntermediateTip'))\n",
        "    connections.append((f'{side}ThumbIntermediateTip', f'{side}ThumbTip'))\n",
        "\n",
        "    for finger in FINGER_NAMES:\n",
        "        connections.append((hand_root, f'{side}{finger}FingerMetacarpal'))\n",
        "        connections.append((f'{side}{finger}FingerMetacarpal', f'{side}{finger}FingerKnuckle'))\n",
        "        connections.append((f'{side}{finger}FingerKnuckle', f'{side}{finger}FingerIntermediateBase'))\n",
        "        connections.append((f'{side}{finger}FingerIntermediateBase', f'{side}{finger}FingerIntermediateTip'))\n",
        "        connections.append((f'{side}{finger}FingerIntermediateTip', f'{side}{finger}FingerTip'))\n",
        "\n",
        "    return connections\n",
        "\n",
        "SKELETON_CONNECTIONS = build_hand_map('left') + build_hand_map('right')\n",
        "\n",
        "def project_point(world_pos, cam_pose, K):\n",
        "    \"\"\"projects 3D world coord to 2D pixels\"\"\"\n",
        "    view_matrix = np.linalg.inv(cam_pose)\n",
        "    point_h = np.append(world_pos, 1) \n",
        "    point_cam = view_matrix @ point_h\n",
        "\n",
        "    if point_cam[2] <= 0: return None\n",
        "\n",
        "    pixel_coords_homo = K @ point_cam[:3]\n",
        "\n",
        "    u = int(pixel_coords_homo[0] / pixel_coords_homo[2])\n",
        "    v = int(pixel_coords_homo[1] / pixel_coords_homo[2])\n",
        "    return (u, v)\n",
        "\n",
        "with h5py.File(H5_FILE, 'r') as f:\n",
        "    K = f['camera']['intrinsic'][:]\n",
        "\n",
        "    cap = cv2.VideoCapture(VIDEO_FILE)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    out = cv2.VideoWriter(OUTPUT_FILE, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    print(\"Drawing Skeleton...\")\n",
        "\n",
        "    frame_idx = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or frame_idx >= 94: break\n",
        "\n",
        "        cam_pose = f['transforms']['camera'][frame_idx]\n",
        "\n",
        "        for start_joint, end_joint in SKELETON_CONNECTIONS:\n",
        "            if start_joint in f['transforms'] and end_joint in f['transforms']:\n",
        "\n",
        "                p1_3d = f['transforms'][start_joint][frame_idx][:3, 3]\n",
        "                p2_3d = f['transforms'][end_joint][frame_idx][:3, 3]\n",
        "\n",
        "                pt1 = project_point(p1_3d, cam_pose, K)\n",
        "                pt2 = project_point(p2_3d, cam_pose, K)\n",
        "\n",
        "                if pt1 and pt2:\n",
        "                    color = (0, 255, 0)\n",
        "                    cv2.line(frame, pt1, pt2, color, 2)\n",
        "                    cv2.circle(frame, pt2, 4, (255, 100, 0), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Skeleton video saved to {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2lQpwpTeOdD"
      },
      "source": [
        "# Camera Trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK94QZ3aeRyL",
        "outputId": "929c4dc3-1c84-4f70-a205-536915826015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Generating 3D paths for: add_remove_lid\n",
            "Generating 3D paths for: basic_pick_and_place\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import glob\n",
        "\n",
        "def generate_all_3d_paths(base_path=\"./\", output_folder=\"video_learning_samples\"):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    tasks = ['add_remove_lid', 'basic_pick_and_place']\n",
        "\n",
        "    for task in tasks:\n",
        "        task_path = os.path.join(base_path, task)\n",
        "        if not os.path.exists(task_path): continue\n",
        "\n",
        "        print(f\"Generating 3D paths for: {task}\")\n",
        "        h5_files = glob.glob(os.path.join(task_path, \"*.hdf5\"))\n",
        "\n",
        "        fig_task = plt.figure(figsize=(10, 8))\n",
        "        ax_task = fig_task.add_subplot(111, projection='3d')\n",
        "\n",
        "        for h5_file in h5_files:\n",
        "            file_id = os.path.splitext(os.path.basename(h5_file))[0]\n",
        "            with h5py.File(h5_file, 'r') as h5:\n",
        "                cam_poses = h5['transforms/camera'][()]\n",
        "                pos = cam_poses[:, :3, 3]\n",
        "\n",
        "                fig_ind = plt.figure(figsize=(8, 6))\n",
        "                ax_ind = fig_ind.add_subplot(111, projection='3d')\n",
        "                ax_ind.plot(pos[:, 0], pos[:, 2], pos[:, 1], color='blue', lw=2)\n",
        "                ax_ind.scatter(pos[0,0], pos[0,2], pos[0,1], color='green', s=100, label='Start')\n",
        "                ax_ind.scatter(pos[-1,0], pos[-1,2], pos[-1,1], color='red', s=100, label='End')\n",
        "                ax_ind.set_title(f\"3D Path: {task} (Video {file_id})\")\n",
        "                ax_ind.set_xlabel('X (meters)')\n",
        "                ax_ind.set_ylabel('Z (meters)')\n",
        "                ax_ind.set_zlabel('Y (meters)')\n",
        "                ax_ind.legend()\n",
        "\n",
        "                plt.savefig(os.path.join(output_folder, f\"{task}_{file_id}_3d_path.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                ax_task.plot(pos[:, 0], pos[:, 2], pos[:, 1], alpha=0.5, label=f\"Trial {file_id}\")\n",
        "\n",
        "        ax_task.set_title(f\"Generalized Trajectory: {task} (All Trials)\")\n",
        "        ax_task.set_xlabel('X (m)')\n",
        "        ax_task.set_ylabel('Z (m)')\n",
        "        ax_task.set_zlabel('Y (m)')\n",
        "        plt.close()\n",
        "\n",
        "generate_all_3d_paths(\"video_learning_samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pw9YKT7mfSb2",
        "outputId": "ad13c6ec-6c5a-4c36-ccb0-912001c792d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Zipping '/content/video_learning_samples'...\n",
            "‚¨áÔ∏è Starting download...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6439c5b7-79cf-4e31-b06b-928d65d6a132\", \"video_learning_samples.zip\", 20413530)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "folder_to_download = \"/content/video_learning_samples\"\n",
        "zip_filename = \"/content/video_learning_samples.zip\"\n",
        "\n",
        "print(f\"Zipping '{folder_to_download}'...\")\n",
        "shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', folder_to_download)\n",
        "\n",
        "print(\"Starting download...\")\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_UIdX2RS5eQ"
      },
      "source": [
        "# PyTorch3D Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuWM-vP8UJIX",
        "outputId": "b6b30c3c-4be7-4e27-e2a3-2fe0826745f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Installing PyTorch3D (This takes ~5 minutes)...\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.2)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath) (4.15.0)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=57534dce78f467d174274287208276323f11a2acb4d9bbaf5efe383fa46e0243\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=7a369be05dbd0e922a774e7555398947e62429086915b200e5f7e227eb7b1468\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n",
            "Collecting git+https://github.com/facebookresearch/pytorch3d.git\n",
            "  Cloning https://github.com/facebookresearch/pytorch3d.git to /tmp/pip-req-build-oypz1c7d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-oypz1c7d\n",
            "  Resolved https://github.com/facebookresearch/pytorch3d.git to commit cbcae096a0b9b04f7c515d11bb4285a82e96b8d7\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.12/dist-packages (from pytorch3d==0.7.9) (0.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from iopath->pytorch3d==0.7.9) (4.67.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from iopath->pytorch3d==0.7.9) (4.15.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath->pytorch3d==0.7.9) (3.2.0)\n",
            "Building wheels for collected packages: pytorch3d\n",
            "  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch3d: filename=pytorch3d-0.7.9-cp312-cp312-linux_x86_64.whl size=60728465 sha256=f6dc34bc01d22b137c6f98853845c40fb2427deec744c4bc16c71c2c576866ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jw3awr3o/wheels/a3/b2/24/3bfb1ad262dd7389a69f1e1aa2afdd7f4f637b3e72dbb9ffa5\n",
            "Successfully built pytorch3d\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.9\n",
            "‚úÖ PyTorch3D Installed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"üîß Installing PyTorch3D (This takes ~5 minutes)...\")\n",
        "\n",
        "!pip install fvcore iopath\n",
        "\n",
        "os.environ['CPLUS_INCLUDE_PATH'] = \"/usr/include/eigen3\"\n",
        "os.environ['TORCH_CUDA_ARCH_LIST'] = \"8.0\"\n",
        "!pip install \"git+https://github.com/facebookresearch/pytorch3d.git\" --no-build-isolation\n",
        "\n",
        "print(\"‚úÖ PyTorch3D Installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU9-CELKtLtQ"
      },
      "source": [
        "# For Foundation Pose -\n",
        "\n",
        "Creating the masks, depth and obj file and creating the 6D tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3rQVvk97saZa",
        "outputId": "9f566aa6-f8e8-4740-c647-bc07e2495ce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining sam_2 from git+https://github.com/facebookresearch/segment-anything-2.git#egg=sam_2\n",
            "  Cloning https://github.com/facebookresearch/segment-anything-2.git to ./src/sam-2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything-2.git /content/src/sam-2\n",
            "  Resolved https://github.com/facebookresearch/segment-anything-2.git to commit 2b90b9f5ceec907a1c18123530e92e794ad901a4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from sam_2) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from sam_2) (0.24.0+cpu)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from sam_2) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from sam_2) (4.67.2)\n",
            "Collecting hydra-core>=1.3.2 (from sam_2)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting iopath>=0.1.10 (from sam_2)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.12/dist-packages (from sam_2) (11.3.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->sam_2) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->sam_2) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.2->sam_2) (26.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.10->sam_2) (4.15.0)\n",
            "Collecting portalocker (from iopath>=0.1.10->sam_2)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->sam_2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->sam_2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->sam_2) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->sam_2) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->sam_2) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.5.1->sam_2) (2025.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->sam_2) (6.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.5.1->sam_2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.5.1->sam_2) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: sam_2, iopath\n",
            "  Building editable for sam_2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sam_2: filename=sam_2-1.0-0.editable-py3-none-any.whl size=13847 sha256=5dbd42b569c294b18239e7f14e2675d5623ab84431a5ed7c34762989fc9e03d2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wey6n1wj/wheels/b0/5b/a1/c16dafb8c3c0046f1158e7fa7570559a72085df5beab759ba0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=105b94d1722110f7661f884ee8944a1958db0a28679e3a7c2ffd9417b2b2547d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built sam_2 iopath\n",
            "Installing collected packages: portalocker, iopath, hydra-core, sam_2\n",
            "Successfully installed hydra-core-1.3.2 iopath-0.1.10 portalocker-3.2.0 sam_2-1.0\n",
            "--2026-02-09 08:37:59--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.169.121.57, 3.169.121.110, 3.169.121.81, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.169.121.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 323493298 (309M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‚Äò./checkpoints/sam2_hiera_base_plus.pt‚Äô\n",
            "\n",
            "sam2_hiera_base_plu 100%[===================>] 308.51M   146MB/s    in 2.1s    \n",
            "\n",
            "2026-02-09 08:38:01 (146 MB/s) - ‚Äò./checkpoints/sam2_hiera_base_plus.pt‚Äô saved [323493298/323493298]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "!pip install -e \"git+https://github.com/facebookresearch/segment-anything-2.git#egg=sam_2\"\n",
        "\n",
        "!wget -P ./checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "1MO0dN-3sfPJ",
        "outputId": "24b0434b-9462-469e-98fd-f1eccbacb70f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "img = Image.open(\"ego_dex_data/rgb/0000.jpg\")\n",
        "plt.imshow(img)\n",
        "plt.grid(True) # find the X and Y coordinates of the object for input in the next part\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tssSXxWEtPCI",
        "outputId": "056ca9e1-67a2-4088-b254-001e46dcf5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAM 2 imported successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:04<00:00, 19.61it/s]\n",
            "/content/src/sam-2/sam2/sam2_video_predictor.py:786: UserWarning: /content/src/sam-2/sam2/_C.so: undefined symbol: _ZNK3c1010TensorImpl15incref_pyobjectEv\n",
            "\n",
            "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
            "  pred_masks_gpu = fill_holes_in_mask_scores(\n",
            "propagate in video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:09<00:00, 10.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully generated 94 masks!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(\"/content/src/sam-2\")\n",
        "\n",
        "try:\n",
        "    from sam2.build_sam import build_sam2_video_predictor\n",
        "    print(\"SAM 2 imported successfully!\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Still not found. Restarting session...\")\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "checkpoint = \"./checkpoints/sam2_hiera_base_plus.pt\"\n",
        "model_cfg = \"sam2_hiera_b+.yaml\" \n",
        "predictor = build_sam2_video_predictor(model_cfg, checkpoint)\n",
        "\n",
        "inference_state = predictor.init_state(video_path=\"ego_dex_data/rgb\")\n",
        "\n",
        "points = np.array([[1300, 900]], dtype=np.float32) #change x,y depending on the plot above.\n",
        "labels = np.array([1], dtype=np.int32)\n",
        "\n",
        "_, out_obj_ids, out_mask_logits = predictor.add_new_points(\n",
        "    inference_state=inference_state,\n",
        "    frame_idx=0,\n",
        "    obj_id=1,\n",
        "    points=points,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "video_segments = {}\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "\n",
        "import cv2\n",
        "os.makedirs(\"ego_dex_data/masks\", exist_ok=True)\n",
        "\n",
        "for frame_idx, obj_masks in video_segments.items():\n",
        "    mask_img = (obj_masks[1][0] * 255).astype(np.uint8)\n",
        "    cv2.imwrite(f\"ego_dex_data/masks/{frame_idx:04d}.jpg\", mask_img)\n",
        "\n",
        "print(\"Successfully generated 94 masks!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6_K4Cr1Afwx",
        "outputId": "35af6cea-9697-43bf-a660-15784f4ac2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting timm==0.6.7\n",
            "  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.6.7) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.6.7) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.7) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.7) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.6.7) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.6.7) (3.0.3)\n",
            "Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.6.7\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (3.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from h5py) (2.0.2)\n",
            "Cloning into 'ZoeDepth'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 114 (delta 40), reused 23 (delta 23), pack-reused 42 (from 2)\u001b[K\n",
            "Receiving objects: 100% (114/114), 4.09 MiB | 36.76 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install timm==0.6.7\n",
        "!pip install h5py tqdm\n",
        "\n",
        "import os\n",
        "if not os.path.exists('ZoeDepth'):\n",
        "    !git clone https://github.com/isl-org/ZoeDepth.git\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/ZoeDepth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSmEdyGBA_Wn",
        "outputId": "744d01d0-8bf9-4ff5-a684-c637ad29d07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted K matrix saved to cam_K.txt\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "with h5py.File('0.hdf5', 'r') as f:\n",
        "    raw_intrinsics = f['camera']['intrinsic'][:]\n",
        "\n",
        "    if raw_intrinsics.shape == (3, 3):\n",
        "        K = raw_intrinsics\n",
        "    elif raw_intrinsics.size == 4:\n",
        "        fx, fy, cx, cy = raw_intrinsics\n",
        "        K = np.array([\n",
        "            [fx, 0,  cx],\n",
        "            [0,  fy, cy],\n",
        "            [0,  0,  1 ]\n",
        "        ])\n",
        "    else:\n",
        "        K = raw_intrinsics\n",
        "\n",
        "    np.savetxt(\"ego_dex_data/cam_K.txt\", K)\n",
        "    print(\"Formatted K matrix saved to cam_K.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrzUcbCBB1cO",
        "outputId": "75a03035-fb44-4872-e69a-c8b512f4fd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ZoeDepth model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/hub.py:335: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/isl-org/ZoeDepth/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "img_size [384, 512]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/hub.py:335: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params passed to Resize transform:\n",
            "\twidth:  512\n",
            "\theight:  384\n",
            "\tresize_target:  True\n",
            "\tkeep_aspect_ratio:  True\n",
            "\tensure_multiple_of:  32\n",
            "\tresize_method:  minimal\n",
            "Using pretrained resource url::https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\n",
            "Downloading: \"https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\" to /root/.cache/torch/hub/checkpoints/ZoeD_M12_N.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [00:07<00:00, 204MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded successfully\n",
            "Starting depth generation for 94 frames...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:35<00:00,  2.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Depth files saved to /content/ego_dex_data/depth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "RGB_DIR = '/content/ego_dex_data/rgb'\n",
        "MASK_DIR = '/content/ego_dex_data/masks'\n",
        "DEPTH_OUT_DIR = '/content/ego_dex_data/depth'\n",
        "os.makedirs(DEPTH_OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Loading ZoeDepth model...\")\n",
        "model_zoe = torch.hub.load(\"isl-org/ZoeDepth\", \"ZoeD_N\", pretrained=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_zoe = model_zoe.to(device).eval()\n",
        "\n",
        "rgb_files = sorted(glob.glob(os.path.join(RGB_DIR, \"*\")))\n",
        "mask_files = sorted(glob.glob(os.path.join(MASK_DIR, \"*\")))\n",
        "\n",
        "if len(rgb_files) != len(mask_files):\n",
        "    print(f\"Warning: Found {len(rgb_files)} RGB images but {len(mask_files)} masks!\")\n",
        "\n",
        "print(f\"Starting depth generation for {len(rgb_files)} frames...\")\n",
        "\n",
        "for i in tqdm(range(len(rgb_files))):\n",
        "    rgb_img = Image.open(rgb_files[i]).convert(\"RGB\")\n",
        "\n",
        "    mask_img = Image.open(mask_files[i]).convert(\"L\")\n",
        "    mask_np = np.array(mask_img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        depth_meters = model_zoe.infer_pil(rgb_img)\n",
        "\n",
        "    depth_mm = (depth_meters * 1000).clip(0, 65535).astype(np.uint16)\n",
        "    depth_mm[mask_np < 128] = 0\n",
        "\n",
        "    file_name = os.path.basename(rgb_files[i]).split('.')[0] + \".png\"\n",
        "    out_path = os.path.join(DEPTH_OUT_DIR, file_name)\n",
        "    Image.fromarray(depth_mm).save(out_path)\n",
        "\n",
        "print(f\"Done! Depth files saved to {DEPTH_OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jVIOTUfaGiA1",
        "outputId": "5b3b66b8-813e-42b6-a9d2-ac2245e6a574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting open3d\n",
            "  Downloading open3d-0.19.0-cp312-cp312-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (2.0.2)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-4.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (3.1.5)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (3.1.2)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (11.3.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.12/dist-packages (from open3d) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.12/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from open3d) (6.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.12/dist-packages (from open3d) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open3d) (4.67.2)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (8.7.1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (4.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (2.32.4)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=3.0.0->open3d) (3.0.3)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.16)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d) (4.26.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7.0->open3d) (5.9.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->open3d) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.30.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.5.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->dash>=2.6.0->open3d) (2026.1.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.5.3)\n",
            "Downloading open3d-0.19.0-cp312-cp312-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-4.0.0-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, widgetsnbextension, retrying, pyquaternion, jedi, configargparse, comm, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 comm-0.2.3 configargparse-1.7.1 dash-4.0.0 ipywidgets-8.1.8 jedi-0.19.2 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.4.2 widgetsnbextension-4.0.15\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "36d86bd0eb054a7da8ce1cb9f4136d59",
              "pip_warning": {
                "packages": [
                  "ipywidgets"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install open3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8UV64WSDXjr",
        "outputId": "5d8c6e14-56a8-4c76-8405-d8265f499778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'FoundationPose'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233 (from 1)\u001b[K\n",
            "Receiving objects: 100% (233/233), 120.80 MiB | 41.08 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "/content/FoundationPose\n",
            "Collecting pycuda\n",
            "  Using cached pycuda-2026.1.tar.gz (1.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transforms3d\n",
            "  Downloading transforms3d-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting trimesh\n",
            "  Downloading trimesh-4.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.2.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pycuda) (4.5.1)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.12/dist-packages (from pycuda) (1.3.10)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from transforms3d) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from scikit-video) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from scikit-video) (1.16.3)\n",
            "Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n",
            "  Downloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from pytools>=2011.2->pycuda) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from mako->pycuda) (3.0.3)\n",
            "Downloading transforms3d-0.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.11.1-py3-none-any.whl (740 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m740.4/740.4 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytools-2025.2.5-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2026.1-cp312-cp312-linux_x86_64.whl size=659448 sha256=c7b99a2b0f149c729bbae86eb1c621f5287c312463f0b51b453fc8dc79677e43\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/2a/71/75ec0cc316cc0ff494bfffa2935e02580129cb7f859a0cfd8f\n",
            "Successfully built pycuda\n",
            "Installing collected packages: trimesh, transforms3d, siphash24, scikit-video, pytools, pycuda\n",
            "Successfully installed pycuda-2026.1 pytools-2025.2.5 scikit-video-1.1.11 siphash24-1.8 transforms3d-0.4.2 trimesh-4.11.1\n",
            "Requirement already satisfied: timm==0.6.7 in /usr/local/lib/python3.12/dist-packages (0.6.7)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.6.7) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.6.7) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.6.7) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.7) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.7) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.6.7) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.6.7) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!rm -r FoundationPose\n",
        "\n",
        "!git clone https://github.com/NVlabs/FoundationPose.git\n",
        "%cd FoundationPose\n",
        "\n",
        "!pip install pycuda transforms3d trimesh scikit-video\n",
        "!pip install timm==0.6.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wgFOa_0MMXyH",
        "outputId": "244e8b16-699d-4b9f-d3e1-66547380bc35"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/FoundationPose/demo_data/my_object/cam_K.txt'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# making the exact folder structure needed for foundation pose demo run\n",
        "target_dir = \"/content/FoundationPose/demo_data/my_object\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "shutil.copytree(\"/content/ego_dex_data/rgb\", f\"{target_dir}/rgb\")\n",
        "shutil.copytree(\"/content/ego_dex_data/depth\", f\"{target_dir}/depth\")\n",
        "shutil.copytree(\"/content/ego_dex_data/masks\", f\"{target_dir}/masks\")\n",
        "\n",
        "shutil.copy(\"/content/ego_dex_data/cam_K.txt\", f\"{target_dir}/cam_K.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTTlLXAFId5r",
        "outputId": "551c8821-a011-47b1-9d4f-d94aaca47ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Refiner weights and config successfully copied!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "refiner_src = '/content/drive/MyDrive/no_diffusion/2023-10-28-18-33-37'\n",
        "refiner_dst = '/content/FoundationPose/weights/2023-10-28-18-33-37'\n",
        "\n",
        "os.makedirs(refiner_dst, exist_ok=True)\n",
        "\n",
        "if os.path.exists(refiner_src):\n",
        "    shutil.copy(f\"{refiner_src}/model_best.pth\", f\"{refiner_dst}/model_best.pth\")\n",
        "    shutil.copy(f\"{refiner_src}/config.yml\", f\"{refiner_dst}/config.yml\")\n",
        "    print(\"Refiner weights and config successfully copied!\")\n",
        "else:\n",
        "    print(\"Error: Could not find the refiner folder in your Drive. Did you add the shortcut?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvRzRLgmL-4g",
        "outputId": "f80a863d-e688-4342-97f2-cc89d053e875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ scorer weights and config successfully copied!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "scorer_src = '/content/drive/MyDrive/no_diffusion/2024-01-11-20-02-45'\n",
        "scorer_dst = '/content/FoundationPose/weights/2024-01-11-20-02-45'\n",
        "\n",
        "os.makedirs(scorer_dst, exist_ok=True)\n",
        "\n",
        "if os.path.exists(scorer_src):\n",
        "    shutil.copy(f\"{scorer_src}/model_best.pth\", f\"{scorer_dst}/model_best.pth\")\n",
        "    shutil.copy(f\"{scorer_src}/config.yml\", f\"{scorer_dst}/config.yml\")\n",
        "    print(\"Scorer weights and config successfully copied!\")\n",
        "else:\n",
        "    print(\"Error: Could not find the scorer folder in your Drive. Did you add the shortcut?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDcJ4om7SKcq",
        "outputId": "2dd8f996-a4da-406f-f12b-006c12a2a89e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Single Frame (Index 0)...\n",
            "SUCCESS! Saved /content/FoundationPose/demo_data/my_object/mesh.obj\n"
          ]
        }
      ],
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "RGB_DIR = '/content/ego_dex_data/rgb'\n",
        "DEPTH_DIR = '/content/ego_dex_data/depth'\n",
        "MASK_DIR = '/content/ego_dex_data/masks'\n",
        "K_PATH = '/content/FoundationPose/demo_data/my_object/cam_K.txt'\n",
        "\n",
        "idx = 0\n",
        "\n",
        "def get_file_at_index(directory, index, ext_list):\n",
        "    for ext in ext_list:\n",
        "        f = sorted(glob.glob(os.path.join(directory, f\"*.{ext}\")))\n",
        "        if len(f) > index: return f[index]\n",
        "    return None\n",
        "\n",
        "rgb_path = get_file_at_index(RGB_DIR, idx, [\"jpg\", \"png\"])\n",
        "depth_path = get_file_at_index(DEPTH_DIR, idx, [\"png\", \"jpg\"])\n",
        "mask_path = get_file_at_index(MASK_DIR, idx, [\"jpg\", \"png\"])\n",
        "\n",
        "print(f\"Processing Single Frame (Index {idx})...\")\n",
        "\n",
        "color = o3d.io.read_image(rgb_path)\n",
        "depth_raw = cv2.imread(depth_path, -1) \n",
        "mask = cv2.imread(mask_path, 0)\n",
        "\n",
        "depth_raw[mask < 100] = 0\n",
        "depth = o3d.geometry.Image(depth_raw)\n",
        "\n",
        "K_orig = np.loadtxt(K_PATH)\n",
        "intrinsic = o3d.camera.PinholeCameraIntrinsic(1920, 1080, K_orig[0,0], K_orig[1,1], K_orig[0,2], K_orig[1,2])\n",
        "\n",
        "rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
        "    color, depth,\n",
        "    depth_scale=1000.0,\n",
        "    depth_trunc=3.0,\n",
        "    convert_rgb_to_intensity=False\n",
        ")\n",
        "\n",
        "\n",
        "volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
        "    voxel_length=0.002,\n",
        "    sdf_trunc=0.01,\n",
        "    color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8\n",
        ")\n",
        "\n",
        "\n",
        "identity_pose = np.eye(4)\n",
        "volume.integrate(rgbd, intrinsic, np.linalg.inv(identity_pose))\n",
        "\n",
        "mesh = volume.extract_triangle_mesh()\n",
        "filename = \"/content/FoundationPose/demo_data/my_object/mesh.obj\"\n",
        "\n",
        "if not mesh.is_empty():\n",
        "    mesh.remove_degenerate_triangles()\n",
        "    mesh.remove_duplicated_vertices()\n",
        "\n",
        "    mesh.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "    o3d.io.write_triangle_mesh(filename, mesh)\n",
        "    print(f\"SUCCESS! Saved {filename}\")\n",
        "else:\n",
        "    print(\"Error: Mesh is empty. Check your mask.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-YyvdZFbU5ee",
        "outputId": "0c95007a-8f6c-4bf6-f53c-f39fbc4dbbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cam_K.txt  depth  masks  mesh.obj  rgb\n",
            "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,341 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,608 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,014 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,607 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [45.0 kB]\n",
            "Get:23 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,297 kB]\n",
            "Get:25 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,712 kB]\n",
            "Get:26 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,388 kB]\n",
            "Get:27 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:28 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,677 kB]\n",
            "Get:29 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,892 kB]\n",
            "Fetched 39.5 MB in 5s (8,549 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libegl-dev libgl-dev libgles-dev libgles1 libglvnd-core-dev libglvnd-dev\n",
            "  libglx-dev libopengl-dev\n",
            "The following NEW packages will be installed:\n",
            "  libegl-dev libegl1-mesa-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n",
            "  libgles2-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev\n",
            "0 upgraded, 11 newly installed, 0 to remove and 119 not upgraded.\n",
            "Need to get 239 kB of archives.\n",
            "After this operation, 2,754 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libegl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [11.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgles2-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [6,854 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [6,848 B]\n",
            "Fetched 239 kB in 0s (1,413 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 11.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../06-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../07-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../08-libegl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libgles2-mesa-dev:amd64.\n",
            "Preparing to unpack .../09-libgles2-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgles2-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../10-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libgles2-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-81.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.46.3)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from wheel) (26.0)\n",
            "Downloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached setuptools-81.0.0-py3-none-any.whl (1.1 MB)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-26.0.1 setuptools-81.0.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b9cda81aaceb4607a1f29f140e31c528",
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 2.9.0+cu126\n",
            "CUDA Available: True\n",
            "‚úÖ GPU Detected: NVIDIA A100-SXM4-80GB\n",
            "CUDA_HOME set to /usr/local/cuda\n",
            "Installing nvdiffrast... (This may take a minute)\n",
            "Collecting git+https://github.com/NVlabs/nvdiffrast.git\n",
            "  Cloning https://github.com/NVlabs/nvdiffrast.git to /tmp/pip-req-build-jusr2izc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVlabs/nvdiffrast.git /tmp/pip-req-build-jusr2izc\n",
            "  Resolved https://github.com/NVlabs/nvdiffrast.git to commit 253ac4fcea7de5f396371124af597e6cc957bfae\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from nvdiffrast==0.4.0) (2.0.2)\n",
            "Building wheels for collected packages: nvdiffrast\n",
            "  Building wheel for nvdiffrast (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvdiffrast: filename=nvdiffrast-0.4.0-cp312-cp312-linux_x86_64.whl size=16595091 sha256=cf1b0d6541f5260ea94dd6ef7a869396349b571c7169a97d464a885c7d9959e4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8onx5k5m/wheels/d1/82/ea/91b5b9219953f7784a69f9e8d2dacad80beb8a99a5e46af62e\n",
            "Successfully built nvdiffrast\n",
            "Installing collected packages: nvdiffrast\n",
            "Successfully installed nvdiffrast-0.4.0\n",
            "DONE! nvdiffrast installed successfully.\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.12/dist-packages (4.11.1)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.12/dist-packages (0.1.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from trimesh) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!ls /content/FoundationPose/demo_data/my_object/\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y libegl1-mesa-dev libgl1-mesa-dev libgles2-mesa-dev\n",
        "\n",
        "!pip install ninja\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"\\nCRITICAL ERROR: You are running on CPU.\")\n",
        "    print(\"Action: Go to 'Runtime' -> 'Change runtime type' -> Select 'T4 GPU' (or A100).\")\n",
        "else:\n",
        "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "    os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
        "    print(\"CUDA_HOME set to /usr/local/cuda\")\n",
        "\n",
        "print(\"Installing nvdiffrast... (This may take a minute)\")\n",
        "!pip install git+https://github.com/NVlabs/nvdiffrast.git --no-build-isolation\n",
        "\n",
        "print(\"DONE! nvdiffrast installed successfully.\")\n",
        "!pip install trimesh yacs pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilBacRD0lb6u",
        "outputId": "458001db-d838-4cdb-92e6-ba45d98fbfff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformations==2024.6.1 in /usr/local/lib/python3.12/dist-packages (2024.6.1)\n",
            "Requirement already satisfied: kornia==0.7.2 in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Requirement already satisfied: einops==0.7.0 in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: warp-lang==1.0.2 in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: roma==1.4.4 in /usr/local/lib/python3.12/dist-packages (1.4.4)\n",
            "Requirement already satisfied: pysdf==0.1.9 in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
            "Requirement already satisfied: trimesh==4.2.2 in /usr/local/lib/python3.12/dist-packages (4.2.2)\n",
            "Requirement already satisfied: pyglet==1.5.28 in /usr/local/lib/python3.12/dist-packages (1.5.28)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from transformations==2024.6.1) (2.0.2)\n",
            "Requirement already satisfied: kornia-rs>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from kornia==0.7.2) (0.1.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia==0.7.2) (26.0)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from kornia==0.7.2) (2.9.0+cu126)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (81.0.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.1->kornia==0.7.2) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9.1->kornia==0.7.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.9.1->kornia==0.7.2) (3.0.3)\n",
            "‚úÖ Installed missing dependencies.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformations==2024.6.1 \\\n",
        "             kornia==0.7.2 \\\n",
        "             einops==0.7.0 \\\n",
        "             warp-lang==1.0.2 \\\n",
        "             roma==1.4.4 \\\n",
        "             pysdf==0.1.9 \\\n",
        "             trimesh==4.2.2 \\\n",
        "             pyglet==1.5.28 \\\n",
        "             omegaconf \\\n",
        "             hydra-core\n",
        "\n",
        "print(\"Installed missing dependencies.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "9XF_myTXmj4n",
        "outputId": "7d168e2c-6b8b-4b36-cc25-304b8097b988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
            "Installing collected packages: ruamel.yaml, numpy\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [numpy]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 ruamel.yaml-0.19.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "4c665bad162e409f9c8ee516cafb1792",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dependencies patched.\n"
          ]
        }
      ],
      "source": [
        "!pip install \"numpy<2.0\" \"ruamel.yaml\"\n",
        "\n",
        "print(\"Dependencies patched.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKaTgzganc_T",
        "outputId": "cd1fb33e-15ad-4d9c-9f7b-6f55a148af6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöú STARTING BULLDOZER SETUP (Ignoring failures)...\n",
            "üîß Installing System Libraries...\n",
            "üì¶ Installing requirements one by one...\n",
            "üëâ Installing --extra-index-url https://download.pytorch.org/whl/cu118... ‚ùå FAILED (Ignoring)\n",
            "‚è© Skipping torch (Use Colab pre-installed version)\n",
            "‚è© Skipping torchvision (Use Colab pre-installed version)\n",
            "‚è© Skipping torchaudio (Use Colab pre-installed version)\n",
            "üëâ Installing jupyterlab... ‚úÖ\n",
            "üëâ Installing ipywidgets... ‚úÖ\n",
            "üëâ Installing numpy... ‚úÖ\n",
            "üëâ Installing scipy... ‚úÖ\n",
            "üëâ Installing scikit-learn... ‚úÖ\n",
            "üëâ Installing scikit-image... ‚úÖ\n",
            "üëâ Installing pyyaml... ‚úÖ\n",
            "üëâ Installing ruamel.yaml... ‚úÖ\n",
            "üëâ Installing ninja... ‚úÖ\n",
            "üëâ Installing h5py... ‚úÖ\n",
            "üëâ Installing numba... ‚úÖ\n",
            "üëâ Installing pybind11... ‚úÖ\n",
            "üëâ Installing imageio... ‚úÖ\n",
            "üëâ Installing opencv-python... ‚úÖ\n",
            "üëâ Installing opencv-contrib-python... ‚úÖ\n",
            "üëâ Installing plotly... ‚úÖ\n",
            "üëâ Installing open3d... ‚úÖ\n",
            "üëâ Installing pyglet... ‚úÖ\n",
            "üëâ Installing pysdf... ‚úÖ\n",
            "üëâ Installing trimesh... ‚úÖ\n",
            "üëâ Installing xatlas... ‚úÖ\n",
            "üëâ Installing rtree... ‚úÖ\n",
            "üëâ Installing pyrender... ‚úÖ\n",
            "üëâ Installing pyOpenGL... ‚úÖ\n",
            "üëâ Installing pyOpenGL_accelerate... ‚úÖ\n",
            "üëâ Installing meshcat... ‚úÖ\n",
            "üëâ Installing webdataset... ‚úÖ\n",
            "üëâ Installing omegaconf... ‚úÖ\n",
            "üëâ Installing pypng... ‚úÖ\n",
            "üëâ Installing Panda3D... ‚úÖ\n",
            "üëâ Installing simplejson... ‚úÖ\n",
            "üëâ Installing bokeh... ‚úÖ\n",
            "üëâ Installing roma... ‚úÖ\n",
            "üëâ Installing seaborn... ‚úÖ\n",
            "üëâ Installing pin... ‚úÖ\n",
            "üëâ Installing openpyxl... ‚úÖ\n",
            "üëâ Installing torchnet... ‚úÖ\n",
            "üëâ Installing wandb... ‚úÖ\n",
            "üëâ Installing colorama... ‚úÖ\n",
            "üëâ Installing GPUtil... ‚úÖ\n",
            "üëâ Installing imgaug... ‚úÖ\n",
            "üëâ Installing xlsxwriter... ‚úÖ\n",
            "üëâ Installing timm... ‚úÖ\n",
            "üëâ Installing albumentations... ‚úÖ\n",
            "üëâ Installing xatlas... ‚úÖ\n",
            "üëâ Installing nodejs... ‚úÖ\n",
            "üëâ Installing jupyterlab... ‚úÖ\n",
            "üëâ Installing objaverse... ‚úÖ\n",
            "üëâ Installing g4f... ‚úÖ\n",
            "üëâ Installing ultralytics... ‚úÖ\n",
            "üëâ Installing pycocotools... ‚úÖ\n",
            "üëâ Installing py-spy... ‚úÖ\n",
            "üëâ Installing pybullet... ‚úÖ\n",
            "üëâ Installing videoio... ‚úÖ\n",
            "üëâ Installing kornia... ‚úÖ\n",
            "üëâ Installing einops... ‚úÖ\n",
            "üëâ Installing transformations... ‚úÖ\n",
            "üëâ Installing joblib... ‚úÖ\n",
            "üëâ Installing warp-lang... ‚úÖ\n",
            "üëâ Installing fvcore... ‚úÖ\n",
            "üì¶ Force installing build tools...\n",
            "üì¶ Force installing NVDiffrast...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import glob\n",
        "import subprocess\n",
        "\n",
        "print(\"STARTING BULLDOZER SETUP (Ignoring failures)...\")\n",
        "base_dir = '/content/FoundationPose'\n",
        "os.chdir(base_dir)\n",
        "\n",
        "print(\"Installing System Libraries...\")\n",
        "subprocess.run(\"sudo apt-get update -qq\", shell=True)\n",
        "subprocess.run(\"sudo apt-get install -y -qq libeigen3-dev libboost-all-dev libgl1-mesa-dev\", shell=True)\n",
        "\n",
        "print(\"Installing requirements one by one...\")\n",
        "with open('requirements.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    if not line or line.startswith('#'): continue\n",
        "\n",
        "    package = line.split('==')[0].split('>=')[0].strip()\n",
        "\n",
        "    if package in ['torch', 'torchvision', 'torchaudio']:\n",
        "        print(f\"Skipping {package} (Use Colab pre-installed version)\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Installing {package}...\", end=\" \")\n",
        "    ret = subprocess.call(f\"pip install {package} -q\", shell=True)\n",
        "    if ret == 0:\n",
        "        print(\"\")\n",
        "    else:\n",
        "        print(\"FAILED (Ignoring)\")\n",
        "\n",
        "print(\"Force installing build tools...\")\n",
        "subprocess.call(\"pip install ninja pybind11 -q\", shell=True)\n",
        "print(\"Force installing NVDiffrast...\")\n",
        "subprocess.call(\"pip install --quiet --no-cache-dir git+https://github.com/NVlabs/nvdiffrast.git\", shell=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ90n11qFMNp",
        "outputId": "e9226c9e-9014-4c33-96f1-3037265f88a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üïµÔ∏è DIAGNOSING THE SILENT IMPORT FAILURE...\n",
            "‚ùå ERROR: mycpp.so is missing!\n",
            "‚úÖ AMAZING! The library imported successfully in this test script.\n",
            "   This suggests the issue is how run_demo_colab.py is finding the file.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"DIAGNOSING THE SILENT IMPORT FAILURE...\")\n",
        "os.chdir('/content/FoundationPose')\n",
        "\n",
        "if '/content/FoundationPose' not in sys.path:\n",
        "    sys.path.append('/content/FoundationPose')\n",
        "\n",
        "if os.path.exists(\"mycpp.so\"):\n",
        "    print(f\"Found library file: mycpp.so ({os.path.getsize('mycpp.so')} bytes)\")\n",
        "else:\n",
        "    print(\"ERROR: mycpp.so is missing!\")\n",
        "\n",
        "try:\n",
        "    print(\"AMAZING! The library imported successfully in this test script.\")\n",
        "    print(\"   This suggests the issue is how run_demo_colab.py is finding the file.\")\n",
        "except ImportError as e:\n",
        "    print(\"\\nCAUGHT IMPORT ERROR:\")\n",
        "    print(f\"   {e}\")\n",
        "    print(\"\\nANALYSIS:\")\n",
        "    err_str = str(e).lower()\n",
        "    if \"numpy\" in err_str:\n",
        "        print(\"This is a NumPy version conflict. The library was compiled with the wrong NumPy header.\")\n",
        "    elif \"symbol\" in err_str:\n",
        "        print(\"This is a Linking Error. The library is missing a system dependency (likely GL or Eigen).\")\n",
        "    elif \"gl\" in err_str or \"render\" in err_str:\n",
        "        print(\"This is a Graphics Driver error. Colab needs a virtual screen.\")\n",
        "    else:\n",
        "        print(\"It's a generic path or dependency issue.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nUNEXPECTED ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EGc6owpmQas",
        "outputId": "c487afdc-7c68-44af-e34c-3427d7b8736d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ CONVERTING JPG -> PNG...\n",
            "   üìÇ Processing 'rgb': Found 94 JPGs\n",
            "   üìÇ Processing 'masks': Found 94 JPGs\n",
            "‚úÖ Successfully converted 188 images to PNG.\n",
            "\n",
            "üßê Verifying file matching...\n",
            "‚úÖ Files matched successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "print(\"CONVERTING JPG -> PNG...\")\n",
        "base_path = \"/content/FoundationPose/demo_data/my_object\"\n",
        "\n",
        "folders_to_check = [\"rgb\", \"depth\", \"masks\"]\n",
        "\n",
        "converted_count = 0\n",
        "\n",
        "for folder in folders_to_check:\n",
        "    current_dir = os.path.join(base_path, folder)\n",
        "    if not os.path.exists(current_dir):\n",
        "        continue\n",
        "\n",
        "    jpg_files = glob.glob(os.path.join(current_dir, \"*.jpg\")) + glob.glob(os.path.join(current_dir, \"*.jpeg\"))\n",
        "\n",
        "    if jpg_files:\n",
        "        print(f\"Processing '{folder}': Found {len(jpg_files)} JPGs\")\n",
        "\n",
        "    for jpg_file in jpg_files:\n",
        "        img = cv2.imread(jpg_file, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Failed to load: {jpg_file}\")\n",
        "            continue\n",
        "\n",
        "        base_name = os.path.splitext(os.path.basename(jpg_file))[0]\n",
        "        png_file = os.path.join(current_dir, base_name + \".png\")\n",
        "\n",
        "        cv2.imwrite(png_file, img)\n",
        "\n",
        "        os.remove(jpg_file)\n",
        "        converted_count += 1\n",
        "\n",
        "if converted_count > 0:\n",
        "    print(f\"Successfully converted {converted_count} images to PNG.\")\n",
        "else:\n",
        "    print(\"No JPGs found (or already converted).\")\n",
        "\n",
        "print(\"\\nVerifying file matching...\")\n",
        "rgb_path = os.path.join(base_path, \"rgb\")\n",
        "depth_path = os.path.join(base_path, \"depth\")\n",
        "masks_path = os.path.join(base_path, \"masks\")\n",
        "\n",
        "rgb_files = sorted(glob.glob(os.path.join(rgb_path, \"*.png\")))\n",
        "\n",
        "if len(rgb_files) == 0:\n",
        "    print(\"ERROR: Still no PNG files found in 'rgb' folder!\")\n",
        "    print(\"Please check your uploads.\")\n",
        "else:\n",
        "    first_rgb = os.path.basename(rgb_files[0])\n",
        "    expected_depth = os.path.join(depth_path, first_rgb)\n",
        "\n",
        "    if not os.path.exists(expected_depth):\n",
        "        print(f\"WARNING: Filename mismatch!\")\n",
        "        print(f\"RGB file:   {first_rgb}\")\n",
        "        print(f\"Missing Depth: {expected_depth}\")\n",
        "        print(\"Make sure your Depth and Mask files have the EXACT same names as your RGB files.\")\n",
        "    else:\n",
        "        print(\"Files matched successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysXhrXuLxh0H",
        "outputId": "b1a94ca7-41df-472f-fe96-f02f41611c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cmeel-boost 1.89.0 requires numpy<2.4,>=2.3; python_version >= \"3.11.0\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "üöÄ LAUNCHING TRACKER...\n",
            "/usr/local/lib/python3.12/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
            "Warp 1.0.2 initialized:\n",
            "   CUDA Toolkit 11.5, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"NVIDIA A100-SXM4-80GB\" (79 GiB, sm_80, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.0.2\n",
            "üìÇ Loading Mesh: demo_data/my_object/mesh.obj\n",
            "[__init__()] self.cfg: \n",
            " lr: 0.0001\n",
            "c_in: 6\n",
            "zfar: 'Infinity'\n",
            "debug: null\n",
            "n_view: 1\n",
            "run_id: 3wy8qqex\n",
            "use_BN: true\n",
            "exp_name: 2024-01-11-20-02-45\n",
            "n_epochs: 62\n",
            "save_dir: /home/bowenw/debug/2024-01-11-20-02-45/\n",
            "use_mask: false\n",
            "loss_type: pairwise_valid\n",
            "optimizer: adam\n",
            "batch_size: 64\n",
            "crop_ratio: 1.1\n",
            "enable_amp: true\n",
            "use_normal: false\n",
            "max_num_key: null\n",
            "warmup_step: -1\n",
            "input_resize:\n",
            "- 160\n",
            "- 160\n",
            "max_step_val: 1000\n",
            "vis_interval: 1000\n",
            "weight_decay: 0\n",
            "normalize_xyz: true\n",
            "resume_run_id: null\n",
            "clip_grad_norm: 'Infinity'\n",
            "lr_epoch_decay: 500\n",
            "render_backend: nvdiffrast\n",
            "train_num_pair: 5\n",
            "lr_decay_epochs:\n",
            "- 50\n",
            "n_epochs_warmup: 1\n",
            "make_pair_online: false\n",
            "gradient_max_norm: 'Infinity'\n",
            "max_step_per_epoch: 10000\n",
            "n_rendering_workers: 1\n",
            "save_epoch_interval: 100\n",
            "n_dataloader_workers: 100\n",
            "split_objects_across_gpus: true\n",
            "ckpt_dir: /content/FoundationPose/learning/training/../../weights/2024-01-11-20-02-45/model_best.pth\n",
            "\n",
            "[__init__()] self.h5_file:None\n",
            "[__init__()] Using pretrained model from /content/FoundationPose/learning/training/../../weights/2024-01-11-20-02-45/model_best.pth\n",
            "[__init__()] init done\n",
            "[__init__()] welcome\n",
            "[__init__()] self.cfg: \n",
            " lr: 0.0001\n",
            "c_in: 6\n",
            "zfar: .inf\n",
            "debug: null\n",
            "w_rot: 0.1\n",
            "n_view: 1\n",
            "run_id: null\n",
            "use_BN: true\n",
            "rot_rep: axis_angle\n",
            "ckpt_dir: /content/FoundationPose/learning/training/../../weights/2023-10-28-18-33-37/model_best.pth\n",
            "exp_name: 2023-10-28-18-33-37\n",
            "save_dir: /tmp/2023-10-28-18-33-37/\n",
            "loss_type: l2\n",
            "optimizer: adam\n",
            "trans_rep: tracknet\n",
            "batch_size: 64\n",
            "crop_ratio: 1.2\n",
            "use_normal: false\n",
            "BN_momentum: 0.1\n",
            "max_num_key: null\n",
            "warmup_step: -1\n",
            "input_resize:\n",
            "- 160\n",
            "- 160\n",
            "max_step_val: 1000\n",
            "normal_uint8: false\n",
            "vis_interval: 1000\n",
            "weight_decay: 0\n",
            "n_max_objects: null\n",
            "normalize_xyz: true\n",
            "clip_grad_norm: 'Infinity'\n",
            "rot_normalizer: 0.3490658503988659\n",
            "trans_normalizer:\n",
            "- 0.019999999552965164\n",
            "- 0.019999999552965164\n",
            "- 0.05000000074505806\n",
            "max_step_per_epoch: 25000\n",
            "val_epoch_interval: 10\n",
            "n_dataloader_workers: 60\n",
            "enable_amp: true\n",
            "use_mask: false\n",
            "\n",
            "[__init__()] self.h5_file:\n",
            "[__init__()] Using pretrained model from /content/FoundationPose/learning/training/../../weights/2023-10-28-18-33-37/model_best.pth\n",
            "[__init__()] init done\n",
            "üß† Initializing FoundationPose...\n",
            "[reset_object()] self.diameter:0.558761784664091, vox_size:0.02793808923320455\n",
            "[reset_object()] self.pts:torch.Size([305, 3])\n",
            "[reset_object()] reset done\n",
            "[make_rotation_grid()] cam_in_obs:(42, 4, 4)\n",
            "[make_rotation_grid()] rot_grid:(252, 4, 4)\n",
            "‚ö†Ô∏è Using Python fallback for pose clustering...\n",
            "[reset_object()] self.diameter:0.5590559181343133, vox_size:0.027952795906715667\n",
            "[reset_object()] self.pts:torch.Size([304, 3])\n",
            "[reset_object()] reset done\n",
            "[make_rotation_grid()] cam_in_obs:(42, 4, 4)\n",
            "[make_rotation_grid()] rot_grid:(252, 4, 4)\n",
            "[make_rotation_grid()] after cluster, rot_grid:(252, 4, 4)\n",
            "[make_rotation_grid()] self.rot_grid: torch.Size([252, 4, 4])\n",
            "üíø Reading Data...\n",
            "üé¨ Starting Tracking Loop (94 frames)...\n",
            "[register()] Welcome\n",
            "Module Utils load on device 'cuda:0' took 626.83 ms\n",
            "[register()] poses:(252, 4, 4)\n",
            "[register()] after viewpoint, add_errs min:-1.0\n",
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1275: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "[predict()] ob_in_cams:(252, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "/content/FoundationPose/learning/training/predict_pose_refine.py:190: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.amp):\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] ob_in_cams:(252, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[find_best_among_pairs()] pose_data.rgbAs.shape[0]: 252\n",
            "/content/FoundationPose/learning/training/predict_score.py:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.amp):\n",
            "[predict()] forward done\n",
            "[register()] final, add_errs min:-1.0\n",
            "[register()] sort ids:tensor([ 67,  27,  30,  58,  87,  96, 102, 108, 109, 113, 184, 191, 223, 236,\n",
            "        240,  28,  31,  59,  93,  94, 178, 183, 235, 241, 185,  68, 186,  35,\n",
            "        237, 173, 177,   5,  64,  70,  71, 105, 131, 202, 219, 220, 226, 227,\n",
            "        229, 232, 238, 242, 244, 250,   3,   4,  14,  33,  75,  86, 130, 201,\n",
            "        225, 230, 249,  48,  57,  63,  76,  99, 100, 107, 206, 243,  26,  43,\n",
            "         49, 150, 157, 190, 193, 208, 214,   9,  19,  38,  41,  45,  55,  65,\n",
            "         72,  85,  92, 118, 136, 151, 156, 171, 175, 181, 197, 203, 221, 233,\n",
            "        247, 251,   2,   6,  17,  18,  44,  46,  51,  54,  60,  61,  80,  89,\n",
            "         90,  91,  97, 104, 127, 128, 129, 134, 135, 140, 159, 160, 162, 164,\n",
            "        168, 169, 179, 180, 199, 209, 210, 217, 218,   8,  16,  20,  40,  56,\n",
            "         62,  74,  79,  82, 110, 119, 120, 121, 122, 124, 126, 132, 147, 165,\n",
            "        174, 176, 187, 192, 198, 211, 215, 216, 239,   0,  11,  13,  24,  37,\n",
            "         42,  95, 115, 116, 125, 139, 144, 145, 182, 200, 245,  25,  69, 153,\n",
            "        166, 195, 204,  29, 188, 205, 212, 246, 143, 103, 114, 137,  15,  53,\n",
            "        146,  34,  39, 123, 138,  77,  78, 149, 155,  12,  22,  50,  83,  84,\n",
            "         98, 106, 111, 148,  10,  47,  52, 133, 142, 228,  88, 163, 167, 196,\n",
            "        207,   7,  21,  73,  81, 117, 152, 161,  23, 141, 154, 158, 172, 194,\n",
            "        213, 224, 231, 234,  36,  66, 248,   1, 101, 112, 189,  32, 170, 222])\n",
            "[register()] sorted scores:tensor([28.8125, 28.7500, 28.7500, 28.7500, 28.7500, 28.7500, 28.7500, 28.7500,\n",
            "        28.7500, 28.7500, 28.7500, 28.7500, 28.7500, 28.7500, 28.7500, 28.6875,\n",
            "        28.6875, 28.6875, 28.6875, 28.6875, 28.6875, 28.6875, 28.6875, 28.6875,\n",
            "        28.6250, 28.4375, 28.4375, 28.2500, 28.1875, 27.2500, 27.1250, 27.0625,\n",
            "        27.0625, 27.0625, 27.0625, 27.0625, 27.0625, 27.0625, 27.0625, 27.0625,\n",
            "        27.0625, 27.0625, 27.0625, 27.0625, 27.0625, 27.0625, 27.0625, 27.0625,\n",
            "        27.0000, 27.0000, 27.0000, 27.0000, 27.0000, 27.0000, 27.0000, 27.0000,\n",
            "        27.0000, 27.0000, 27.0000, 26.9375, 26.9375, 26.9375, 26.9375, 26.9375,\n",
            "        26.9375, 26.9375, 26.9375, 26.9375, 26.8750, 26.8750, 26.8750, 26.8750,\n",
            "        26.8750, 26.8750, 26.8750, 26.8750, 26.8750, 26.8125, 26.8125, 26.8125,\n",
            "        26.8125, 26.8125, 26.8125, 26.8125, 26.8125, 26.8125, 26.8125, 26.8125,\n",
            "        26.8125, 26.8125, 26.8125, 26.8125, 26.8125, 26.8125, 26.8125, 26.8125,\n",
            "        26.8125, 26.8125, 26.8125, 26.8125, 26.7500, 26.7500, 26.7500, 26.7500,\n",
            "        26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500,\n",
            "        26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500,\n",
            "        26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500,\n",
            "        26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.7500, 26.6875,\n",
            "        26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875,\n",
            "        26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875,\n",
            "        26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875, 26.6875,\n",
            "        26.6875, 26.6875, 26.6875, 26.6250, 26.6250, 26.6250, 26.6250, 26.6250,\n",
            "        26.6250, 26.6250, 26.6250, 26.6250, 26.6250, 26.6250, 26.6250, 26.6250,\n",
            "        26.6250, 26.6250, 26.6250, 26.5625, 26.5625, 26.5625, 26.5625, 26.5625,\n",
            "        26.5625, 26.5000, 26.5000, 26.5000, 26.5000, 26.4375, 26.3750, 26.3125,\n",
            "        26.3125, 26.3125, 26.2500, 26.2500, 26.1875, 26.1250, 26.1250, 26.1250,\n",
            "        26.1250, 26.0625, 26.0625, 26.0625, 26.0000, 25.9375, 25.9375, 25.9375,\n",
            "        25.9375, 25.9375, 25.9375, 25.9375, 25.9375, 25.9375, 25.8750, 25.8750,\n",
            "        25.8750, 25.8750, 25.8750, 25.8750, 25.8125, 25.8125, 25.8125, 25.8125,\n",
            "        25.8125, 25.7500, 25.7500, 25.7500, 25.7500, 25.7500, 25.7500, 25.7500,\n",
            "        25.6875, 25.6875, 25.6875, 25.6875, 25.6875, 25.6875, 25.6875, 25.6875,\n",
            "        25.6875, 25.6875, 25.6250, 25.6250, 25.6250, 25.5625, 25.5625, 25.5625,\n",
            "        25.5625, 25.5000, 25.5000, 25.5000])\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 10...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 20...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 30...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 40...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 50...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 60...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 70...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 80...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "   Processing frame 90...\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "[track_one()] Welcome\n",
            "[track_one()] depth processing done\n",
            "[predict()] ob_in_cams:(1, 4, 4)\n",
            "[predict()] self.cfg.use_normal:False\n",
            "[predict()] trans_normalizer:[0.019999999552965164, 0.019999999552965164, 0.05000000074505806], rot_normalizer:0.3490658503988659\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[predict()] making cropped data\n",
            "[make_crop_data_batch()] Welcome make_crop_data_batch\n",
            "[make_crop_data_batch()] make tf_to_crops done\n",
            "[make_crop_data_batch()] render done\n",
            "[make_crop_data_batch()] warp done\n",
            "[make_crop_data_batch()] pose batch data done\n",
            "[predict()] forward start\n",
            "[predict()] forward done\n",
            "[track_one()] pose done\n",
            "‚úÖ DONE! Frames saved to 'debug/track_vis'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "!pip install \"numpy<2.0\" --quiet\n",
        "\n",
        "base_dir = '/content/FoundationPose'\n",
        "os.chdir(base_dir)\n",
        "\n",
        "patch_code = \"\"\"\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import trimesh\n",
        "import argparse\n",
        "import logging\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    import mycpp\n",
        "    if not hasattr(mycpp, 'cluster_poses'):\n",
        "        for attr in dir(mycpp):\n",
        "            sub_obj = getattr(mycpp, attr)\n",
        "            if hasattr(sub_obj, 'cluster_poses'):\n",
        "                mycpp.cluster_poses = sub_obj.cluster_poses\n",
        "                break\n",
        "except Exception as e:\n",
        "    pass\n",
        "\n",
        "import estimater\n",
        "estimater.mycpp = mycpp\n",
        "\n",
        "from estimater import FoundationPose, ScorePredictor, PoseRefinePredictor\n",
        "from datareader import YcbineoatReader\n",
        "import nvdiffrast.torch as dr\n",
        "\n",
        "if __name__=='__main__':\n",
        "  mesh_file = 'demo_data/my_object/mesh.obj'\n",
        "  test_scene_dir = 'demo_data/my_object'\n",
        "  debug_dir = 'debug'\n",
        "\n",
        "  if not os.path.exists(mesh_file):\n",
        "      print(f\"CRITICAL ERROR: Could not find {mesh_file}\")\n",
        "      print(\"Please check the folder 'demo_data/my_object' to see if the file is actually there.\")\n",
        "      sys.exit(1)\n",
        "\n",
        "  print(f\"Loading Mesh: {mesh_file}\")\n",
        "  mesh = trimesh.load(mesh_file)\n",
        "\n",
        "  if isinstance(mesh, trimesh.Scene):\n",
        "      print(\"Loaded as Scene. Extracting geometry...\")\n",
        "      if len(mesh.geometry) == 0:\n",
        "          print(\"Error: Mesh file is empty!\")\n",
        "          sys.exit(1)\n",
        "      mesh = trimesh.util.concatenate(tuple(mesh.geometry.values()))\n",
        "\n",
        "  os.system(f'rm -rf {debug_dir}/* && mkdir -p {debug_dir}/track_vis {debug_dir}/ob_in_cam')\n",
        "\n",
        "  to_origin, extents = trimesh.bounds.oriented_bounds(mesh)\n",
        "  bbox = np.stack([-extents/2, extents/2], axis=0).reshape(2,3)\n",
        "\n",
        "  scorer = ScorePredictor()\n",
        "  refiner = PoseRefinePredictor()\n",
        "  glctx = dr.RasterizeCudaContext()\n",
        "\n",
        "  print(\"Initializing FoundationPose...\")\n",
        "\n",
        "  try:\n",
        "      est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=1, glctx=glctx)\n",
        "  except AttributeError:\n",
        "      print(\"Using Python fallback for pose clustering...\")\n",
        "      mycpp.cluster_poses = lambda dist, thr, poses, syms: poses\n",
        "      est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=1, glctx=glctx)\n",
        "\n",
        "  print(\"Reading Data...\")\n",
        "  reader = YcbineoatReader(video_dir=test_scene_dir, shorter_side=None, zfar=np.inf)\n",
        "\n",
        "  print(f\"Starting Tracking Loop ({len(reader.color_files)} frames)...\")\n",
        "  for i in range(len(reader.color_files)):\n",
        "    color = reader.get_color(i)\n",
        "    depth = reader.get_depth(i)\n",
        "\n",
        "    if i==0:\n",
        "      mask = reader.get_mask(0).astype(bool)\n",
        "      pose = est.register(K=reader.K, rgb=color, depth=depth, ob_mask=mask, iteration=5)\n",
        "    else:\n",
        "      if i % 10 == 0: print(f\"   Processing frame {i}...\")\n",
        "      pose = est.track_one(rgb=color, depth=depth, K=reader.K, iteration=2)\n",
        "\n",
        "    output_path = f'{debug_dir}/track_vis/{reader.id_strs[i]}.png'\n",
        "    center_pose = pose@np.linalg.inv(to_origin)\n",
        "    vis = estimater.draw_posed_3d_box(reader.K, img=color, ob_in_cam=center_pose, bbox=bbox)\n",
        "    vis = estimater.draw_xyz_axis(color, ob_in_cam=center_pose, scale=0.1, K=reader.K, thickness=3, transparency=0, is_input_rgb=True)\n",
        "    cv2.imwrite(output_path, vis[...,::-1])\n",
        "\n",
        "  print(f\"DONE! Frames saved to '{debug_dir}/track_vis'\")\n",
        "\"\"\"\n",
        "\n",
        "with open('run_demo_patched.py', 'w') as f:\n",
        "    f.write(patch_code)\n",
        "\n",
        "print(\"\\nLAUNCHING TRACKER...\")\n",
        "os.environ['PYTHONPATH'] = base_dir\n",
        "!python run_demo_patched.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "E5b57gCEO8II",
        "outputId": "0400a532-9c1b-4cb4-b955-d322518d6d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Tracking Complete! Zipping results...\n",
            "‚¨áÔ∏è Downloading 'tracking_results_2.zip' to your computer...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_9ee05750-78e3-4021-977e-5e7e63f207af\", \"tracking_results_2.zip\", 86825155)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "results_dir = \"/content/FoundationPose/debug/track_vis\"\n",
        "if os.path.exists(results_dir) and len(os.listdir(results_dir)) > 0:\n",
        "    print(\"\\nTracking Complete! Zipping results...\")\n",
        "\n",
        "    output_filename = \"/content/tracking_results_2\"\n",
        "    shutil.make_archive(output_filename, 'zip', results_dir)\n",
        "\n",
        "    print(\"Downloading 'tracking_results_2.zip' to your computer...\")\n",
        "    files.download(output_filename + \".zip\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nError: No results found. The tracker might have failed or the 'rgb' folder is still empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "wihVLQAwdP1p",
        "outputId": "ee564dd0-6d8b-40fc-8230-609f3842a720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Tracking Complete! Zipping results...\n",
            "‚¨áÔ∏è Downloading 'demo_data_2.zip' to your computer...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f7bb09be-ee5e-4fa2-a03b-c2c2b654bc69\", \"demo_data_2.zip\", 87394197)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "results_dir = \"/content/FoundationPose/demo_data\"\n",
        "if os.path.exists(results_dir) and len(os.listdir(results_dir)) > 0:\n",
        "    print(\"\\nTracking Complete! Zipping results...\")\n",
        "\n",
        "    output_filename = \"/content/demo_data_2\"\n",
        "    shutil.make_archive(output_filename, 'zip', results_dir)\n",
        "\n",
        "    print(\"Downloading 'demo_data_2.zip' to your computer...\")\n",
        "    files.download(output_filename + \".zip\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nError: No results found. The tracker might have failed or the 'rgb' folder is still empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJTXQyVDs5Fr"
      },
      "source": [
        "# For CoTracker3 -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EppagSW2dArr",
        "outputId": "95949c47-e086-4203-de69-4656c430a328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'co-tracker'...\n",
            "remote: Enumerating objects: 447, done.\u001b[K\n",
            "remote: Counting objects: 100% (273/273), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 447 (delta 204), reused 129 (delta 129), pack-reused 174 (from 2)\u001b[K\n",
            "Receiving objects: 100% (447/447), 64.52 MiB | 43.55 MiB/s, done.\n",
            "Resolving deltas: 100% (245/245), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/co-tracker.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJDQmQGWd4W9",
        "outputId": "a2bab275-a073-4768-a524-70de6e2d14de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/co-tracker\n",
            "Obtaining file:///content/co-tracker\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: cotracker\n",
            "  Running setup.py develop for cotracker\n",
            "Successfully installed cotracker-3.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting flow_vis\n",
            "  Downloading flow_vis-0.1-py3-none-any.whl.metadata (731 bytes)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10.1)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Downloading flow_vis-0.1-py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: flow_vis\n",
            "Successfully installed flow_vis-0.1\n",
            "--2026-02-05 07:20:32--  https://huggingface.co/facebook/cotracker3/resolve/main/scaled_offline.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.170.185.35, 3.170.185.25, 3.170.185.14, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.170.185.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://us.gcp.cdn.hf.co/xet-bridge-us/66f28e717967278698d7ebf7/ae694a923aeae71dcdbb5b3ecba33e3c224d2b10b6ffb5dd5ba50c0ab3cdcc45?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27scaled_offline.pth%3B+filename%3D%22scaled_offline.pth%22%3B&Expires=1770279632&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzcwMjc5NjMyfX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjZmMjhlNzE3OTY3Mjc4Njk4ZDdlYmY3L2FlNjk0YTkyM2FlYWU3MWRjZGJiNWIzZWNiYTMzZTNjMjI0ZDJiMTBiNmZmYjVkZDViYTUwYzBhYjNjZGNjNDVcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=qJQKZ0SynWagX1tAhfWKT4yx2FVxGvduz9R9XnNkMNnuEJNwc-nfo7Jxl%7E7%7EDTSK%7Egn6XqDY6kmsRjA%7EERL4qgrkuPQ8ZFsPcEEvOe%7EDK-wjcXwT-fZdD2zHQT4owKqB6hJQqjOeNYpfjgP0vozyl3YsPypN5Im2b3kFL7LZLZb0ayQlxToRy6tpj-glkOiXBrHKe4OUCjPIFVECRZOcCOqSkEMdgNc82hjIIi-rOP5gd1ggVX0Qi4QX4NxEwndG0AgirzyoJ2anGvpKwaWeM1OWVu-oLTwGEPf6fKCke3kf7MY921kALj4Sns5do6lfqQOfKzc3iLThIlaCjb56hQ__&Key-Pair-Id=KJLH8B0YWU4Y8M [following]\n",
            "--2026-02-05 07:20:32--  https://us.gcp.cdn.hf.co/xet-bridge-us/66f28e717967278698d7ebf7/ae694a923aeae71dcdbb5b3ecba33e3c224d2b10b6ffb5dd5ba50c0ab3cdcc45?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27scaled_offline.pth%3B+filename%3D%22scaled_offline.pth%22%3B&Expires=1770279632&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzcwMjc5NjMyfX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjZmMjhlNzE3OTY3Mjc4Njk4ZDdlYmY3L2FlNjk0YTkyM2FlYWU3MWRjZGJiNWIzZWNiYTMzZTNjMjI0ZDJiMTBiNmZmYjVkZDViYTUwYzBhYjNjZGNjNDVcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=qJQKZ0SynWagX1tAhfWKT4yx2FVxGvduz9R9XnNkMNnuEJNwc-nfo7Jxl%7E7%7EDTSK%7Egn6XqDY6kmsRjA%7EERL4qgrkuPQ8ZFsPcEEvOe%7EDK-wjcXwT-fZdD2zHQT4owKqB6hJQqjOeNYpfjgP0vozyl3YsPypN5Im2b3kFL7LZLZb0ayQlxToRy6tpj-glkOiXBrHKe4OUCjPIFVECRZOcCOqSkEMdgNc82hjIIi-rOP5gd1ggVX0Qi4QX4NxEwndG0AgirzyoJ2anGvpKwaWeM1OWVu-oLTwGEPf6fKCke3kf7MY921kALj4Sns5do6lfqQOfKzc3iLThIlaCjb56hQ__&Key-Pair-Id=KJLH8B0YWU4Y8M\n",
            "Resolving us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)... 34.120.165.110\n",
            "Connecting to us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)|34.120.165.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 101890938 (97M) [application/octet-stream]\n",
            "Saving to: ‚Äòcheckpoints/scaled_offline.pth‚Äô\n",
            "\n",
            "scaled_offline.pth  100%[===================>]  97.17M  78.1MB/s    in 1.2s    \n",
            "\n",
            "2026-02-05 07:20:33 (78.1 MB/s) - ‚Äòcheckpoints/scaled_offline.pth‚Äô saved [101890938/101890938]\n",
            "\n",
            "Installation and download complete.\n"
          ]
        }
      ],
      "source": [
        "%cd co-tracker\n",
        "\n",
        "!pip install -e .\n",
        "!pip install matplotlib flow_vis tqdm tensorboard\n",
        "\n",
        "# 3. Create checkpoints directory and download CoTracker3 weights\n",
        "# We download 'scaled_offline.pth' which is the high-accuracy model for whole videos\n",
        "!mkdir -p checkpoints\n",
        "!wget -P checkpoints https://huggingface.co/facebook/cotracker3/resolve/main/scaled_offline.pth\n",
        "\n",
        "# Download online model if you prefer sliding window later\n",
        "# !wget -P checkpoints https://huggingface.co/facebook/cotracker3/resolve/main/scaled_online.pth\n",
        "\n",
        "print(\"Installation and download complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5RKEgifeHu4",
        "outputId": "d2067563-ff7a-46f7-8259-f45fbdbdfde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Truncating video from 413 to 100 frames.\n",
            "Final video shape for model: torch.Size([1, 100, 3, 1080, 1920])\n",
            "Saving data to HDF5...\n",
            "Coordinates saved to: /content/pick_place_cotracker3.h5\n",
            "Generating visualization video...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video saved to /content/pick_place_cotracker3.mp4\n",
            "Video saved to: /content/pick_place_cotracker3.mp4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "from cotracker.predictor import CoTrackerPredictor\n",
        "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
        "\n",
        "video_path = \"/content/0.mp4\"\n",
        "checkpoint_path = \"./checkpoints/scaled_offline.pth\"\n",
        "output_h5 = \"/content/pick_place_cotracker3.h5\"\n",
        "output_vis = \"/content/pick_place_cotracker3.mp4\"\n",
        "\n",
        "def run_extraction():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = CoTrackerPredictor(checkpoint=checkpoint_path)\n",
        "    model = model.to(device)\n",
        "\n",
        "    video = read_video_from_path(video_path)\n",
        "\n",
        "    if isinstance(video, np.ndarray):\n",
        "        video = torch.from_numpy(video)\n",
        "\n",
        "    if video.ndim == 4:\n",
        "        video = video.unsqueeze(0)\n",
        "\n",
        "    MAX_FRAMES = 100\n",
        "    if video.shape[1] > MAX_FRAMES:\n",
        "        print(f\"Truncating video from {video.shape[1]} to {MAX_FRAMES} frames.\")\n",
        "        video = video[:, :MAX_FRAMES]\n",
        "\n",
        "    if video.shape[-1] == 3:\n",
        "        video = video.permute(0, 1, 4, 2, 3)\n",
        "\n",
        "    video = video.to(device).float()\n",
        "    print(f\"Final video shape for model: {video.shape}\")\n",
        "\n",
        "    pred_tracks, pred_visibility = model(\n",
        "        video,\n",
        "        grid_size=60,\n",
        "        grid_query_frame=0\n",
        "    )\n",
        "\n",
        "    print(\"Saving data to HDF5...\")\n",
        "    with h5py.File(output_h5, 'w') as f:\n",
        "        f.create_dataset(\"tracks\", data=pred_tracks[0].cpu().numpy())\n",
        "        f.create_dataset(\"visibility\", data=pred_visibility[0].cpu().numpy())\n",
        "\n",
        "    print(f\"Coordinates saved to: {output_h5}\")\n",
        "\n",
        "    print(\"Generating visualization video...\")\n",
        "    vis = Visualizer(save_dir=\"/content\", linewidth=2)\n",
        "    vis.visualize(\n",
        "        video,\n",
        "        pred_tracks,\n",
        "        pred_visibility,\n",
        "        filename=\"pick_place_cotracker3\"\n",
        "    )\n",
        "    print(f\"Video saved to: {output_vis}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_extraction()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsnCcbnJb-r"
      },
      "source": [
        "# HaMeR: Hand Mesh Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4HKySQWfoYy",
        "outputId": "675aa59d-b903-45f1-c95c-bdb290e590c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'hamer/': No such file or directory\n",
            "‚¨áÔ∏è Cloning HaMeR...\n",
            "Cloning into 'hamer'...\n",
            "remote: Enumerating objects: 271, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 271 (delta 82), reused 54 (delta 54), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (271/271), 3.96 MiB | 57.90 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "Submodule 'third-party/ViTPose' (https://github.com/ViTAE-Transformer/ViTPose.git) registered for path 'third-party/ViTPose'\n",
            "Cloning into '/content/hamer/hamer/third-party/ViTPose'...\n",
            "remote: Enumerating objects: 1868, done.        \n",
            "remote: Counting objects: 100% (8/8), done.        \n",
            "remote: Compressing objects: 100% (8/8), done.        \n",
            "remote: Total 1868 (delta 4), reused 0 (delta 0), pack-reused 1860 (from 2)        \n",
            "Receiving objects: 100% (1868/1868), 10.75 MiB | 11.70 MiB/s, done.\n",
            "Resolving deltas: 100% (977/977), done.\n",
            "Submodule path 'third-party/ViTPose': checked out 'd5216452796c90c6bc29f5c5ec0bdba94366768a'\n",
            "‚öôÔ∏è Installing Dependencies...\n",
            "/content/hamer/hamer\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2 /tmp/pip-install-khxovjms/detectron2_6daf580c44604a009f6f7ae227c838c6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mattloper/chumpy /tmp/pip-install-khxovjms/chumpy_10658ad78d0042dcb6dcae184b4a2c44\n",
            "  Running command python setup.py egg_info\n",
            "  /usr/local/lib/python3.12/dist-packages/setuptools/dist.py:765: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "\n",
            "          License :: OSI Approved :: Apache Software License\n",
            "\n",
            "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    self._finalize_license_expression()\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no files found matching 'mmpose/.mim/model-index.yml'\n",
            "  warning: no files found matching '*.py' under directory 'mmpose/.mim/configs'\n",
            "  warning: no files found matching '*.yml' under directory 'mmpose/.mim/configs'\n",
            "  warning: no files found matching '*.py' under directory 'mmpose/.mim/tools'\n",
            "  warning: no files found matching '*.sh' under directory 'mmpose/.mim/tools'\n",
            "  warning: no files found matching '*.py' under directory 'mmpose/.mim/demo'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-tcsk6bzs/mmpose.egg-info/SOURCES.txt'\n",
            "    Running command python setup.py develop\n",
            "    /usr/local/lib/python3.12/dist-packages/setuptools/dist.py:765: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "\n",
            "            License :: OSI Approved :: Apache Software License\n",
            "\n",
            "            See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      self._finalize_license_expression()\n",
            "    running develop\n",
            "    /usr/local/lib/python3.12/dist-packages/setuptools/_distutils/cmd.py:90: DevelopDeprecationWarning: develop command is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` and ``develop``.\n",
            "            Instead, use standards-based tools like pip or uv.\n",
            "\n",
            "            This deprecation is overdue, please update your project and remove deprecated\n",
            "            calls to avoid build errors in the future.\n",
            "\n",
            "            See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      self.initialize_options()\n",
            "    WARNING: Ignoring invalid distribution ~mpose (/usr/local/lib/python3.12/dist-packages)\n",
            "    Obtaining file:///content/hamer/hamer/third-party/ViTPose\n",
            "      Installing build dependencies: started\n",
            "      Installing build dependencies: finished with status 'done'\n",
            "      Checking if build backend supports build_editable: started\n",
            "      Checking if build backend supports build_editable: finished with status 'done'\n",
            "      Getting requirements to build editable: started\n",
            "      Getting requirements to build editable: finished with status 'done'\n",
            "      Preparing editable metadata (pyproject.toml): started\n",
            "      Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "      WARNING: Ignoring invalid distribution ~mpose (/usr/local/lib/python3.12/dist-packages)\n",
            "    Building wheels for collected packages: mmpose\n",
            "      Building editable for mmpose (pyproject.toml): started\n",
            "      Building editable for mmpose (pyproject.toml): finished with status 'done'\n",
            "      Created wheel for mmpose: filename=mmpose-0.24.0-0.editable-py2.py3-none-any.whl size=12465 sha256=bea5257d75deb7a2e14e35fd3b18630e61454f72409e5a0a68143c203ad448fa\n",
            "      Stored in directory: /tmp/pip-ephem-wheel-cache-i_q6jelw/wheels/9d/f6/58/6d5d8e8505a9cb4c710a6225ae4a8c1b6b445558a53a4ab486\n",
            "    Successfully built mmpose\n",
            "    Installing collected packages: mmpose\n",
            "    Successfully installed mmpose-0.24.0\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1mv7CUAnm73oKsEEG1xE3xH2C_oqcFSzT\n",
            "From (redirected): https://drive.google.com/uc?id=1mv7CUAnm73oKsEEG1xE3xH2C_oqcFSzT&confirm=t&uuid=2ff49c2a-81eb-47de-9087-9b86da004580\n",
            "To: /content/hamer/hamer/hamer_demo_data.tar.gz\n",
            "100% 6.04G/6.04G [00:52<00:00, 115MB/s]\n",
            "_DATA/\n",
            "_DATA/vitpose_ckpts/\n",
            "_DATA/vitpose_ckpts/vitpose+_huge/\n",
            "_DATA/vitpose_ckpts/vitpose+_huge/wholebody.pth\n",
            "_DATA/data/\n",
            "_DATA/data/mano_mean_params.npz\n",
            "_DATA/data/mano/\n",
            "_DATA/hamer_ckpts/\n",
            "_DATA/hamer_ckpts/model_config.yaml\n",
            "_DATA/hamer_ckpts/dataset_config.yaml\n",
            "_DATA/hamer_ckpts/checkpoints/\n",
            "_DATA/hamer_ckpts/checkpoints/hamer.ckpt\n",
            "\n",
            "üì¶ Copying MANO files from Drive...\n",
            "   ‚úÖ Found and moved MANO_RIGHT.pkl\n",
            "   ‚úÖ Found and moved MANO_LEFT.pkl\n",
            "\n",
            "‚úÖ INSTALLATION COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "!rm -r hamer/\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"Cloning HaMeR...\")\n",
        "!git clone --recursive https://github.com/geopavlakos/hamer.git\n",
        "\n",
        "print(\"Installing Dependencies...\")\n",
        "%cd hamer\n",
        "!pip install -e .[all] > /dev/null\n",
        "!pip install -v -e third-party/ViTPose > /dev/null\n",
        "!bash fetch_demo_data.sh\n",
        "\n",
        "dest_mano = \"/content/hamer/_DATA/data/mano\"\n",
        "os.makedirs(dest_mano, exist_ok=True)\n",
        "\n",
        "drive_mano = \"/content/drive/MyDrive/EgoDex_Data/mano_models\"\n",
        "files_to_copy = [\"MANO_RIGHT.pkl\", \"MANO_LEFT.pkl\"]\n",
        "\n",
        "print(\"\\nCopying MANO files from Drive...\")\n",
        "for f in files_to_copy:\n",
        "    src = os.path.join(drive_mano, f)\n",
        "    dst = os.path.join(dest_mano, f)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, dst)\n",
        "        print(f\"Found and moved {f}\")\n",
        "    else:\n",
        "        print(f\"WARNING: Could not find {f} in {drive_mano}\")\n",
        "        print(\"Did you upload it to the 'mano_models' folder in Drive?\")\n",
        "\n",
        "print(\"\\nINSTALLATION COMPLETE.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9q0k1Se-5uY",
        "outputId": "1e987777-7105-4d20-b6a7-68516cddf099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found HaMeR repo. Starting processing on: /content/ego_dex_data/rgb\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/mmcv/utils/parrots_wrapper.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import parse_version\n",
            "apex is not installed\n",
            "apex is not installed\n",
            "apex is not installed\n",
            "/usr/local/lib/python3.12/dist-packages/mmcv/cnn/bricks/transformer.py:27: UserWarning: Fail to import ``MultiScaleDeformableAttention`` from ``mmcv.ops.multi_scale_deform_attn``, You should install ``mmcv-full`` if you need this module. \n",
            "  warnings.warn('Fail to import ``MultiScaleDeformableAttention`` from '\n",
            "/usr/local/lib/python3.12/dist-packages/lightning_fabric/utilities/cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "WARNING: You are using a MANO model, with only 10 shape coefficients.\n",
            "Use load_from_local loader\n",
            "/usr/local/lib/python3.12/dist-packages/mmcv/runner/checkpoint.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, map_location=map_location)\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "unexpected key in source state_dict: backbone.blocks.0.mlp.experts.0.weight, backbone.blocks.0.mlp.experts.0.bias, backbone.blocks.0.mlp.experts.1.weight, backbone.blocks.0.mlp.experts.1.bias, backbone.blocks.0.mlp.experts.2.weight, backbone.blocks.0.mlp.experts.2.bias, backbone.blocks.0.mlp.experts.3.weight, backbone.blocks.0.mlp.experts.3.bias, backbone.blocks.0.mlp.experts.4.weight, backbone.blocks.0.mlp.experts.4.bias, backbone.blocks.0.mlp.experts.5.weight, backbone.blocks.0.mlp.experts.5.bias, backbone.blocks.1.mlp.experts.0.weight, backbone.blocks.1.mlp.experts.0.bias, backbone.blocks.1.mlp.experts.1.weight, backbone.blocks.1.mlp.experts.1.bias, backbone.blocks.1.mlp.experts.2.weight, backbone.blocks.1.mlp.experts.2.bias, backbone.blocks.1.mlp.experts.3.weight, backbone.blocks.1.mlp.experts.3.bias, backbone.blocks.1.mlp.experts.4.weight, backbone.blocks.1.mlp.experts.4.bias, backbone.blocks.1.mlp.experts.5.weight, backbone.blocks.1.mlp.experts.5.bias, backbone.blocks.2.mlp.experts.0.weight, backbone.blocks.2.mlp.experts.0.bias, backbone.blocks.2.mlp.experts.1.weight, backbone.blocks.2.mlp.experts.1.bias, backbone.blocks.2.mlp.experts.2.weight, backbone.blocks.2.mlp.experts.2.bias, backbone.blocks.2.mlp.experts.3.weight, backbone.blocks.2.mlp.experts.3.bias, backbone.blocks.2.mlp.experts.4.weight, backbone.blocks.2.mlp.experts.4.bias, backbone.blocks.2.mlp.experts.5.weight, backbone.blocks.2.mlp.experts.5.bias, backbone.blocks.3.mlp.experts.0.weight, backbone.blocks.3.mlp.experts.0.bias, backbone.blocks.3.mlp.experts.1.weight, backbone.blocks.3.mlp.experts.1.bias, backbone.blocks.3.mlp.experts.2.weight, backbone.blocks.3.mlp.experts.2.bias, backbone.blocks.3.mlp.experts.3.weight, backbone.blocks.3.mlp.experts.3.bias, backbone.blocks.3.mlp.experts.4.weight, backbone.blocks.3.mlp.experts.4.bias, backbone.blocks.3.mlp.experts.5.weight, backbone.blocks.3.mlp.experts.5.bias, backbone.blocks.4.mlp.experts.0.weight, backbone.blocks.4.mlp.experts.0.bias, backbone.blocks.4.mlp.experts.1.weight, backbone.blocks.4.mlp.experts.1.bias, backbone.blocks.4.mlp.experts.2.weight, backbone.blocks.4.mlp.experts.2.bias, backbone.blocks.4.mlp.experts.3.weight, backbone.blocks.4.mlp.experts.3.bias, backbone.blocks.4.mlp.experts.4.weight, backbone.blocks.4.mlp.experts.4.bias, backbone.blocks.4.mlp.experts.5.weight, backbone.blocks.4.mlp.experts.5.bias, backbone.blocks.5.mlp.experts.0.weight, backbone.blocks.5.mlp.experts.0.bias, backbone.blocks.5.mlp.experts.1.weight, backbone.blocks.5.mlp.experts.1.bias, backbone.blocks.5.mlp.experts.2.weight, backbone.blocks.5.mlp.experts.2.bias, backbone.blocks.5.mlp.experts.3.weight, backbone.blocks.5.mlp.experts.3.bias, backbone.blocks.5.mlp.experts.4.weight, backbone.blocks.5.mlp.experts.4.bias, backbone.blocks.5.mlp.experts.5.weight, backbone.blocks.5.mlp.experts.5.bias, backbone.blocks.6.mlp.experts.0.weight, backbone.blocks.6.mlp.experts.0.bias, backbone.blocks.6.mlp.experts.1.weight, backbone.blocks.6.mlp.experts.1.bias, backbone.blocks.6.mlp.experts.2.weight, backbone.blocks.6.mlp.experts.2.bias, backbone.blocks.6.mlp.experts.3.weight, backbone.blocks.6.mlp.experts.3.bias, backbone.blocks.6.mlp.experts.4.weight, backbone.blocks.6.mlp.experts.4.bias, backbone.blocks.6.mlp.experts.5.weight, backbone.blocks.6.mlp.experts.5.bias, backbone.blocks.7.mlp.experts.0.weight, backbone.blocks.7.mlp.experts.0.bias, backbone.blocks.7.mlp.experts.1.weight, backbone.blocks.7.mlp.experts.1.bias, backbone.blocks.7.mlp.experts.2.weight, backbone.blocks.7.mlp.experts.2.bias, backbone.blocks.7.mlp.experts.3.weight, backbone.blocks.7.mlp.experts.3.bias, backbone.blocks.7.mlp.experts.4.weight, backbone.blocks.7.mlp.experts.4.bias, backbone.blocks.7.mlp.experts.5.weight, backbone.blocks.7.mlp.experts.5.bias, backbone.blocks.8.mlp.experts.0.weight, backbone.blocks.8.mlp.experts.0.bias, backbone.blocks.8.mlp.experts.1.weight, backbone.blocks.8.mlp.experts.1.bias, backbone.blocks.8.mlp.experts.2.weight, backbone.blocks.8.mlp.experts.2.bias, backbone.blocks.8.mlp.experts.3.weight, backbone.blocks.8.mlp.experts.3.bias, backbone.blocks.8.mlp.experts.4.weight, backbone.blocks.8.mlp.experts.4.bias, backbone.blocks.8.mlp.experts.5.weight, backbone.blocks.8.mlp.experts.5.bias, backbone.blocks.9.mlp.experts.0.weight, backbone.blocks.9.mlp.experts.0.bias, backbone.blocks.9.mlp.experts.1.weight, backbone.blocks.9.mlp.experts.1.bias, backbone.blocks.9.mlp.experts.2.weight, backbone.blocks.9.mlp.experts.2.bias, backbone.blocks.9.mlp.experts.3.weight, backbone.blocks.9.mlp.experts.3.bias, backbone.blocks.9.mlp.experts.4.weight, backbone.blocks.9.mlp.experts.4.bias, backbone.blocks.9.mlp.experts.5.weight, backbone.blocks.9.mlp.experts.5.bias, backbone.blocks.10.mlp.experts.0.weight, backbone.blocks.10.mlp.experts.0.bias, backbone.blocks.10.mlp.experts.1.weight, backbone.blocks.10.mlp.experts.1.bias, backbone.blocks.10.mlp.experts.2.weight, backbone.blocks.10.mlp.experts.2.bias, backbone.blocks.10.mlp.experts.3.weight, backbone.blocks.10.mlp.experts.3.bias, backbone.blocks.10.mlp.experts.4.weight, backbone.blocks.10.mlp.experts.4.bias, backbone.blocks.10.mlp.experts.5.weight, backbone.blocks.10.mlp.experts.5.bias, backbone.blocks.11.mlp.experts.0.weight, backbone.blocks.11.mlp.experts.0.bias, backbone.blocks.11.mlp.experts.1.weight, backbone.blocks.11.mlp.experts.1.bias, backbone.blocks.11.mlp.experts.2.weight, backbone.blocks.11.mlp.experts.2.bias, backbone.blocks.11.mlp.experts.3.weight, backbone.blocks.11.mlp.experts.3.bias, backbone.blocks.11.mlp.experts.4.weight, backbone.blocks.11.mlp.experts.4.bias, backbone.blocks.11.mlp.experts.5.weight, backbone.blocks.11.mlp.experts.5.bias, backbone.blocks.12.mlp.experts.0.weight, backbone.blocks.12.mlp.experts.0.bias, backbone.blocks.12.mlp.experts.1.weight, backbone.blocks.12.mlp.experts.1.bias, backbone.blocks.12.mlp.experts.2.weight, backbone.blocks.12.mlp.experts.2.bias, backbone.blocks.12.mlp.experts.3.weight, backbone.blocks.12.mlp.experts.3.bias, backbone.blocks.12.mlp.experts.4.weight, backbone.blocks.12.mlp.experts.4.bias, backbone.blocks.12.mlp.experts.5.weight, backbone.blocks.12.mlp.experts.5.bias, backbone.blocks.13.mlp.experts.0.weight, backbone.blocks.13.mlp.experts.0.bias, backbone.blocks.13.mlp.experts.1.weight, backbone.blocks.13.mlp.experts.1.bias, backbone.blocks.13.mlp.experts.2.weight, backbone.blocks.13.mlp.experts.2.bias, backbone.blocks.13.mlp.experts.3.weight, backbone.blocks.13.mlp.experts.3.bias, backbone.blocks.13.mlp.experts.4.weight, backbone.blocks.13.mlp.experts.4.bias, backbone.blocks.13.mlp.experts.5.weight, backbone.blocks.13.mlp.experts.5.bias, backbone.blocks.14.mlp.experts.0.weight, backbone.blocks.14.mlp.experts.0.bias, backbone.blocks.14.mlp.experts.1.weight, backbone.blocks.14.mlp.experts.1.bias, backbone.blocks.14.mlp.experts.2.weight, backbone.blocks.14.mlp.experts.2.bias, backbone.blocks.14.mlp.experts.3.weight, backbone.blocks.14.mlp.experts.3.bias, backbone.blocks.14.mlp.experts.4.weight, backbone.blocks.14.mlp.experts.4.bias, backbone.blocks.14.mlp.experts.5.weight, backbone.blocks.14.mlp.experts.5.bias, backbone.blocks.15.mlp.experts.0.weight, backbone.blocks.15.mlp.experts.0.bias, backbone.blocks.15.mlp.experts.1.weight, backbone.blocks.15.mlp.experts.1.bias, backbone.blocks.15.mlp.experts.2.weight, backbone.blocks.15.mlp.experts.2.bias, backbone.blocks.15.mlp.experts.3.weight, backbone.blocks.15.mlp.experts.3.bias, backbone.blocks.15.mlp.experts.4.weight, backbone.blocks.15.mlp.experts.4.bias, backbone.blocks.15.mlp.experts.5.weight, backbone.blocks.15.mlp.experts.5.bias, backbone.blocks.16.mlp.experts.0.weight, backbone.blocks.16.mlp.experts.0.bias, backbone.blocks.16.mlp.experts.1.weight, backbone.blocks.16.mlp.experts.1.bias, backbone.blocks.16.mlp.experts.2.weight, backbone.blocks.16.mlp.experts.2.bias, backbone.blocks.16.mlp.experts.3.weight, backbone.blocks.16.mlp.experts.3.bias, backbone.blocks.16.mlp.experts.4.weight, backbone.blocks.16.mlp.experts.4.bias, backbone.blocks.16.mlp.experts.5.weight, backbone.blocks.16.mlp.experts.5.bias, backbone.blocks.17.mlp.experts.0.weight, backbone.blocks.17.mlp.experts.0.bias, backbone.blocks.17.mlp.experts.1.weight, backbone.blocks.17.mlp.experts.1.bias, backbone.blocks.17.mlp.experts.2.weight, backbone.blocks.17.mlp.experts.2.bias, backbone.blocks.17.mlp.experts.3.weight, backbone.blocks.17.mlp.experts.3.bias, backbone.blocks.17.mlp.experts.4.weight, backbone.blocks.17.mlp.experts.4.bias, backbone.blocks.17.mlp.experts.5.weight, backbone.blocks.17.mlp.experts.5.bias, backbone.blocks.18.mlp.experts.0.weight, backbone.blocks.18.mlp.experts.0.bias, backbone.blocks.18.mlp.experts.1.weight, backbone.blocks.18.mlp.experts.1.bias, backbone.blocks.18.mlp.experts.2.weight, backbone.blocks.18.mlp.experts.2.bias, backbone.blocks.18.mlp.experts.3.weight, backbone.blocks.18.mlp.experts.3.bias, backbone.blocks.18.mlp.experts.4.weight, backbone.blocks.18.mlp.experts.4.bias, backbone.blocks.18.mlp.experts.5.weight, backbone.blocks.18.mlp.experts.5.bias, backbone.blocks.19.mlp.experts.0.weight, backbone.blocks.19.mlp.experts.0.bias, backbone.blocks.19.mlp.experts.1.weight, backbone.blocks.19.mlp.experts.1.bias, backbone.blocks.19.mlp.experts.2.weight, backbone.blocks.19.mlp.experts.2.bias, backbone.blocks.19.mlp.experts.3.weight, backbone.blocks.19.mlp.experts.3.bias, backbone.blocks.19.mlp.experts.4.weight, backbone.blocks.19.mlp.experts.4.bias, backbone.blocks.19.mlp.experts.5.weight, backbone.blocks.19.mlp.experts.5.bias, backbone.blocks.20.mlp.experts.0.weight, backbone.blocks.20.mlp.experts.0.bias, backbone.blocks.20.mlp.experts.1.weight, backbone.blocks.20.mlp.experts.1.bias, backbone.blocks.20.mlp.experts.2.weight, backbone.blocks.20.mlp.experts.2.bias, backbone.blocks.20.mlp.experts.3.weight, backbone.blocks.20.mlp.experts.3.bias, backbone.blocks.20.mlp.experts.4.weight, backbone.blocks.20.mlp.experts.4.bias, backbone.blocks.20.mlp.experts.5.weight, backbone.blocks.20.mlp.experts.5.bias, backbone.blocks.21.mlp.experts.0.weight, backbone.blocks.21.mlp.experts.0.bias, backbone.blocks.21.mlp.experts.1.weight, backbone.blocks.21.mlp.experts.1.bias, backbone.blocks.21.mlp.experts.2.weight, backbone.blocks.21.mlp.experts.2.bias, backbone.blocks.21.mlp.experts.3.weight, backbone.blocks.21.mlp.experts.3.bias, backbone.blocks.21.mlp.experts.4.weight, backbone.blocks.21.mlp.experts.4.bias, backbone.blocks.21.mlp.experts.5.weight, backbone.blocks.21.mlp.experts.5.bias, backbone.blocks.22.mlp.experts.0.weight, backbone.blocks.22.mlp.experts.0.bias, backbone.blocks.22.mlp.experts.1.weight, backbone.blocks.22.mlp.experts.1.bias, backbone.blocks.22.mlp.experts.2.weight, backbone.blocks.22.mlp.experts.2.bias, backbone.blocks.22.mlp.experts.3.weight, backbone.blocks.22.mlp.experts.3.bias, backbone.blocks.22.mlp.experts.4.weight, backbone.blocks.22.mlp.experts.4.bias, backbone.blocks.22.mlp.experts.5.weight, backbone.blocks.22.mlp.experts.5.bias, backbone.blocks.23.mlp.experts.0.weight, backbone.blocks.23.mlp.experts.0.bias, backbone.blocks.23.mlp.experts.1.weight, backbone.blocks.23.mlp.experts.1.bias, backbone.blocks.23.mlp.experts.2.weight, backbone.blocks.23.mlp.experts.2.bias, backbone.blocks.23.mlp.experts.3.weight, backbone.blocks.23.mlp.experts.3.bias, backbone.blocks.23.mlp.experts.4.weight, backbone.blocks.23.mlp.experts.4.bias, backbone.blocks.23.mlp.experts.5.weight, backbone.blocks.23.mlp.experts.5.bias, backbone.blocks.24.mlp.experts.0.weight, backbone.blocks.24.mlp.experts.0.bias, backbone.blocks.24.mlp.experts.1.weight, backbone.blocks.24.mlp.experts.1.bias, backbone.blocks.24.mlp.experts.2.weight, backbone.blocks.24.mlp.experts.2.bias, backbone.blocks.24.mlp.experts.3.weight, backbone.blocks.24.mlp.experts.3.bias, backbone.blocks.24.mlp.experts.4.weight, backbone.blocks.24.mlp.experts.4.bias, backbone.blocks.24.mlp.experts.5.weight, backbone.blocks.24.mlp.experts.5.bias, backbone.blocks.25.mlp.experts.0.weight, backbone.blocks.25.mlp.experts.0.bias, backbone.blocks.25.mlp.experts.1.weight, backbone.blocks.25.mlp.experts.1.bias, backbone.blocks.25.mlp.experts.2.weight, backbone.blocks.25.mlp.experts.2.bias, backbone.blocks.25.mlp.experts.3.weight, backbone.blocks.25.mlp.experts.3.bias, backbone.blocks.25.mlp.experts.4.weight, backbone.blocks.25.mlp.experts.4.bias, backbone.blocks.25.mlp.experts.5.weight, backbone.blocks.25.mlp.experts.5.bias, backbone.blocks.26.mlp.experts.0.weight, backbone.blocks.26.mlp.experts.0.bias, backbone.blocks.26.mlp.experts.1.weight, backbone.blocks.26.mlp.experts.1.bias, backbone.blocks.26.mlp.experts.2.weight, backbone.blocks.26.mlp.experts.2.bias, backbone.blocks.26.mlp.experts.3.weight, backbone.blocks.26.mlp.experts.3.bias, backbone.blocks.26.mlp.experts.4.weight, backbone.blocks.26.mlp.experts.4.bias, backbone.blocks.26.mlp.experts.5.weight, backbone.blocks.26.mlp.experts.5.bias, backbone.blocks.27.mlp.experts.0.weight, backbone.blocks.27.mlp.experts.0.bias, backbone.blocks.27.mlp.experts.1.weight, backbone.blocks.27.mlp.experts.1.bias, backbone.blocks.27.mlp.experts.2.weight, backbone.blocks.27.mlp.experts.2.bias, backbone.blocks.27.mlp.experts.3.weight, backbone.blocks.27.mlp.experts.3.bias, backbone.blocks.27.mlp.experts.4.weight, backbone.blocks.27.mlp.experts.4.bias, backbone.blocks.27.mlp.experts.5.weight, backbone.blocks.27.mlp.experts.5.bias, backbone.blocks.28.mlp.experts.0.weight, backbone.blocks.28.mlp.experts.0.bias, backbone.blocks.28.mlp.experts.1.weight, backbone.blocks.28.mlp.experts.1.bias, backbone.blocks.28.mlp.experts.2.weight, backbone.blocks.28.mlp.experts.2.bias, backbone.blocks.28.mlp.experts.3.weight, backbone.blocks.28.mlp.experts.3.bias, backbone.blocks.28.mlp.experts.4.weight, backbone.blocks.28.mlp.experts.4.bias, backbone.blocks.28.mlp.experts.5.weight, backbone.blocks.28.mlp.experts.5.bias, backbone.blocks.29.mlp.experts.0.weight, backbone.blocks.29.mlp.experts.0.bias, backbone.blocks.29.mlp.experts.1.weight, backbone.blocks.29.mlp.experts.1.bias, backbone.blocks.29.mlp.experts.2.weight, backbone.blocks.29.mlp.experts.2.bias, backbone.blocks.29.mlp.experts.3.weight, backbone.blocks.29.mlp.experts.3.bias, backbone.blocks.29.mlp.experts.4.weight, backbone.blocks.29.mlp.experts.4.bias, backbone.blocks.29.mlp.experts.5.weight, backbone.blocks.29.mlp.experts.5.bias, backbone.blocks.30.mlp.experts.0.weight, backbone.blocks.30.mlp.experts.0.bias, backbone.blocks.30.mlp.experts.1.weight, backbone.blocks.30.mlp.experts.1.bias, backbone.blocks.30.mlp.experts.2.weight, backbone.blocks.30.mlp.experts.2.bias, backbone.blocks.30.mlp.experts.3.weight, backbone.blocks.30.mlp.experts.3.bias, backbone.blocks.30.mlp.experts.4.weight, backbone.blocks.30.mlp.experts.4.bias, backbone.blocks.30.mlp.experts.5.weight, backbone.blocks.30.mlp.experts.5.bias, backbone.blocks.31.mlp.experts.0.weight, backbone.blocks.31.mlp.experts.0.bias, backbone.blocks.31.mlp.experts.1.weight, backbone.blocks.31.mlp.experts.1.bias, backbone.blocks.31.mlp.experts.2.weight, backbone.blocks.31.mlp.experts.2.bias, backbone.blocks.31.mlp.experts.3.weight, backbone.blocks.31.mlp.experts.3.bias, backbone.blocks.31.mlp.experts.4.weight, backbone.blocks.31.mlp.experts.4.bias, backbone.blocks.31.mlp.experts.5.weight, backbone.blocks.31.mlp.experts.5.bias\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "downsampling_factor=np.float32(1.5377142)\n",
            "downsampling_factor=np.float32(1.4807627)\n",
            "downsampling_factor=np.float32(1.2272263)\n",
            "downsampling_factor=np.float32(2.37264)\n",
            "/content/hamer/hamer/hamer/utils/geometry.py:61: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
            "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
            "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)\n",
            "  b3 = torch.cross(b1, b2)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "[ WARN:0@57.712] global loadsave.cpp:1089 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
            "downsampling_factor=np.float32(1.3901536)\n",
            "downsampling_factor=np.float32(1.5639231)\n",
            "downsampling_factor=np.float32(1.5072733)\n",
            "downsampling_factor=np.float32(2.4493206)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.1699781)\n",
            "downsampling_factor=np.float32(1.5599695)\n",
            "downsampling_factor=np.float32(1.0334396)\n",
            "downsampling_factor=np.float32(0.91861343)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(0.92272186)\n",
            "downsampling_factor=np.float32(0.92272186)\n",
            "downsampling_factor=np.float32(1.1661816)\n",
            "downsampling_factor=np.float32(1.295756)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.3980598)\n",
            "downsampling_factor=np.float32(1.464634)\n",
            "downsampling_factor=np.float32(2.7857068)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4111881)\n",
            "downsampling_factor=np.float32(1.4783874)\n",
            "downsampling_factor=np.float32(1.9775162)\n",
            "downsampling_factor=np.float32(2.448354)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.6298548)\n",
            "downsampling_factor=np.float32(2.3161085)\n",
            "downsampling_factor=np.float32(1.5756775)\n",
            "downsampling_factor=np.float32(1.512651)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.3386828)\n",
            "downsampling_factor=np.float32(1.4502398)\n",
            "downsampling_factor=np.float32(1.3619099)\n",
            "downsampling_factor=np.float32(1.5564703)\n",
            "downsampling_factor=np.float32(1.5367451)\n",
            "downsampling_factor=np.float32(1.606597)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4803507)\n",
            "downsampling_factor=np.float32(1.542032)\n",
            "downsampling_factor=np.float32(1.9376272)\n",
            "downsampling_factor=np.float32(2.289922)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5310752)\n",
            "downsampling_factor=np.float32(1.5877817)\n",
            "downsampling_factor=np.float32(1.7696406)\n",
            "downsampling_factor=np.float32(2.477496)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4231751)\n",
            "downsampling_factor=np.float32(1.4824741)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4382195)\n",
            "downsampling_factor=np.float32(1.495748)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4515986)\n",
            "downsampling_factor=np.float32(1.5096626)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4788568)\n",
            "downsampling_factor=np.float32(1.5380108)\n",
            "downsampling_factor=np.float32(1.5560735)\n",
            "downsampling_factor=np.float32(2.1968105)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.17033)\n",
            "downsampling_factor=np.float32(1.5604376)\n",
            "downsampling_factor=np.float32(1.2717133)\n",
            "downsampling_factor=np.float32(1.1561025)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.1711397)\n",
            "downsampling_factor=np.float32(1.8217748)\n",
            "downsampling_factor=np.float32(1.0291876)\n",
            "downsampling_factor=np.float32(1.0291876)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(2.8156269)\n",
            "downsampling_factor=np.float32(2.627918)\n",
            "downsampling_factor=np.float32(1.4932854)\n",
            "downsampling_factor=np.float32(1.5507195)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(0.9346593)\n",
            "downsampling_factor=np.float32(1.2851562)\n",
            "downsampling_factor=np.float32(1.1691236)\n",
            "downsampling_factor=np.float32(1.8186365)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.0361023)\n",
            "downsampling_factor=np.float32(1.0361023)\n",
            "downsampling_factor=np.float32(1.3584614)\n",
            "downsampling_factor=np.float32(1.2937723)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4655218)\n",
            "downsampling_factor=np.float32(1.4655218)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(0.9217046)\n",
            "downsampling_factor=np.float32(0.9217046)\n",
            "downsampling_factor=np.float32(1.1692495)\n",
            "downsampling_factor=np.float32(1.5590006)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4916334)\n",
            "downsampling_factor=np.float32(1.4916334)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.3865457)\n",
            "downsampling_factor=np.float32(1.5071158)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.7558594)\n",
            "downsampling_factor=np.float32(2.1690037)\n",
            "downsampling_factor=np.float32(1.4387293)\n",
            "downsampling_factor=np.float32(1.5757512)\n",
            "downsampling_factor=np.float32(1.3491964)\n",
            "downsampling_factor=np.float32(2.3129096)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4743209)\n",
            "downsampling_factor=np.float32(1.5332937)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.7573624)\n",
            "downsampling_factor=np.float32(2.050256)\n",
            "downsampling_factor=np.float32(1.4952335)\n",
            "downsampling_factor=np.float32(1.5664349)\n",
            "downsampling_factor=np.float32(1.1605282)\n",
            "downsampling_factor=np.float32(2.0631638)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.521246)\n",
            "downsampling_factor=np.float32(1.521246)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.1709566)\n",
            "downsampling_factor=np.float32(1.8214874)\n",
            "downsampling_factor=np.float32(1.1589762)\n",
            "downsampling_factor=np.float32(1.0430781)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.708388)\n",
            "downsampling_factor=np.float32(2.028712)\n",
            "downsampling_factor=np.float32(1.3605652)\n",
            "downsampling_factor=np.float32(1.8140844)\n",
            "downsampling_factor=np.float32(1.5154314)\n",
            "downsampling_factor=np.float32(1.6531981)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.521483)\n",
            "downsampling_factor=np.float32(1.521483)\n",
            "downsampling_factor=np.float32(1.8619919)\n",
            "downsampling_factor=np.float32(1.4767532)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5078878)\n",
            "downsampling_factor=np.float32(1.5078878)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.1698608)\n",
            "downsampling_factor=np.float32(1.5598145)\n",
            "downsampling_factor=np.float32(0.9180959)\n",
            "downsampling_factor=np.float32(0.9180959)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.7577286)\n",
            "downsampling_factor=np.float32(2.8678715)\n",
            "downsampling_factor=np.float32(1.4794364)\n",
            "downsampling_factor=np.float32(1.5363383)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.6988119)\n",
            "downsampling_factor=np.float32(2.4538407)\n",
            "downsampling_factor=np.float32(1.4427297)\n",
            "downsampling_factor=np.float32(1.5581481)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.479662)\n",
            "downsampling_factor=np.float32(1.6503924)\n",
            "downsampling_factor=np.float32(2.0169907)\n",
            "downsampling_factor=np.float32(2.7504425)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4433594)\n",
            "downsampling_factor=np.float32(1.9588445)\n",
            "downsampling_factor=np.float32(1.49688)\n",
            "downsampling_factor=np.float32(1.49688)\n",
            "downsampling_factor=np.float32(1.3342838)\n",
            "downsampling_factor=np.float32(2.0331929)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.1710911)\n",
            "downsampling_factor=np.float32(1.5614573)\n",
            "downsampling_factor=np.float32(1.0329462)\n",
            "downsampling_factor=np.float32(1.0329462)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4968834)\n",
            "downsampling_factor=np.float32(1.5451698)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.8895632)\n",
            "downsampling_factor=np.float32(2.287365)\n",
            "downsampling_factor=np.float32(1.5007629)\n",
            "downsampling_factor=np.float32(1.5689797)\n",
            "downsampling_factor=np.float32(1.3565464)\n",
            "downsampling_factor=np.float32(2.0671184)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.2668394)\n",
            "downsampling_factor=np.float32(1.1516724)\n",
            "downsampling_factor=np.float32(1.1685677)\n",
            "downsampling_factor=np.float32(1.298411)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5185647)\n",
            "downsampling_factor=np.float32(1.5185647)\n",
            "downsampling_factor=np.float32(1.3933424)\n",
            "downsampling_factor=np.float32(1.6255671)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(2.7559102)\n",
            "downsampling_factor=np.float32(1.5697247)\n",
            "downsampling_factor=np.float32(1.5697247)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5729464)\n",
            "downsampling_factor=np.float32(1.5729464)\n",
            "downsampling_factor=np.float32(1.4221052)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4696732)\n",
            "downsampling_factor=np.float32(1.5221615)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5266838)\n",
            "downsampling_factor=np.float32(1.6357332)\n",
            "downsampling_factor=np.float32(1.5260506)\n",
            "downsampling_factor=np.float32(1.5954161)\n",
            "downsampling_factor=np.float32(1.3554115)\n",
            "downsampling_factor=np.float32(1.5490417)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.456892)\n",
            "downsampling_factor=np.float32(1.5734428)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5515203)\n",
            "downsampling_factor=np.float32(1.5515203)\n",
            "downsampling_factor=np.float32(1.8363571)\n",
            "downsampling_factor=np.float32(2.2735848)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5098238)\n",
            "downsampling_factor=np.float32(1.558527)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4219971)\n",
            "downsampling_factor=np.float32(1.4897113)\n",
            "downsampling_factor=np.float32(1.8506228)\n",
            "downsampling_factor=np.float32(2.4350307)\n",
            "downsampling_factor=np.float32(1.3562355)\n",
            "downsampling_factor=np.float32(2.3249714)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4678854)\n",
            "downsampling_factor=np.float32(1.2420579)\n",
            "downsampling_factor=np.float32(1.5383329)\n",
            "downsampling_factor=np.float32(1.5383329)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.484843)\n",
            "downsampling_factor=np.float32(1.5990617)\n",
            "downsampling_factor=np.float32(1.6799355)\n",
            "downsampling_factor=np.float32(2.7065647)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.491602)\n",
            "downsampling_factor=np.float32(1.5512662)\n",
            "downsampling_factor=np.float32(1.6295522)\n",
            "downsampling_factor=np.float32(2.0822055)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4503717)\n",
            "downsampling_factor=np.float32(1.4503717)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4695911)\n",
            "downsampling_factor=np.float32(1.5283747)\n",
            "downsampling_factor=np.float32(1.9611562)\n",
            "downsampling_factor=np.float32(2.2413204)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5178199)\n",
            "downsampling_factor=np.float32(1.5178199)\n",
            "downsampling_factor=np.float32(1.608017)\n",
            "downsampling_factor=np.float32(2.1440225)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.2483393)\n",
            "downsampling_factor=np.float32(1.2483393)\n",
            "downsampling_factor=np.float32(1.3647499)\n",
            "downsampling_factor=np.float32(1.2997614)\n",
            "downsampling_factor=np.float32(1.4895997)\n",
            "downsampling_factor=np.float32(1.560533)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4276304)\n",
            "downsampling_factor=np.float32(1.4276304)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5127864)\n",
            "downsampling_factor=np.float32(1.4470129)\n",
            "downsampling_factor=np.float32(2.7896526)\n",
            "downsampling_factor=np.float32(0.9730263)\n",
            "downsampling_factor=np.float32(2.5947368)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4738872)\n",
            "downsampling_factor=np.float32(1.5352991)\n",
            "downsampling_factor=np.float32(1.8603579)\n",
            "downsampling_factor=np.float32(2.3033001)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(2.5521393)\n",
            "downsampling_factor=np.float32(2.2786968)\n",
            "downsampling_factor=np.float32(1.5475001)\n",
            "downsampling_factor=np.float32(1.6178406)\n",
            "downsampling_factor=np.float32(1.1612148)\n",
            "downsampling_factor=np.float32(2.3224335)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.0405846)\n",
            "downsampling_factor=np.float32(0.92496365)\n",
            "downsampling_factor=np.float32(1.3599825)\n",
            "downsampling_factor=np.float32(1.5542654)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4678488)\n",
            "downsampling_factor=np.float32(1.4678488)\n",
            "downsampling_factor=np.float32(2.0699666)\n",
            "downsampling_factor=np.float32(2.6099598)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4523096)\n",
            "downsampling_factor=np.float32(1.4523096)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.3953464)\n",
            "downsampling_factor=np.float32(1.511626)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(2.633334)\n",
            "downsampling_factor=np.float32(2.5483882)\n",
            "downsampling_factor=np.float32(1.5289383)\n",
            "downsampling_factor=np.float32(1.5984354)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4942341)\n",
            "downsampling_factor=np.float32(1.5424356)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4632099)\n",
            "downsampling_factor=np.float32(1.5241768)\n",
            "downsampling_factor=np.float32(1.6901487)\n",
            "downsampling_factor=np.float32(2.2238808)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.471848)\n",
            "downsampling_factor=np.float32(1.5284576)\n",
            "downsampling_factor=np.float32(1.1992289)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.0386301)\n",
            "downsampling_factor=np.float32(2.0772603)\n",
            "downsampling_factor=np.float32(1.1484896)\n",
            "downsampling_factor=np.float32(1.6078848)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4717255)\n",
            "downsampling_factor=np.float32(1.5207834)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(2.8996174)\n",
            "downsampling_factor=np.float32(1.5512505)\n",
            "downsampling_factor=np.float32(1.6186962)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(2.7532475)\n",
            "downsampling_factor=np.float32(2.6745834)\n",
            "downsampling_factor=np.float32(1.5172997)\n",
            "downsampling_factor=np.float32(1.5862684)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5161223)\n",
            "downsampling_factor=np.float32(1.5161223)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.3635225)\n",
            "downsampling_factor=np.float32(1.8180288)\n",
            "downsampling_factor=np.float32(1.5710449)\n",
            "downsampling_factor=np.float32(1.6832606)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4439948)\n",
            "downsampling_factor=np.float32(1.501755)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4378805)\n",
            "downsampling_factor=np.float32(1.4953957)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.3948908)\n",
            "downsampling_factor=np.float32(1.4555376)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4046662)\n",
            "downsampling_factor=np.float32(1.5217221)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4593992)\n",
            "downsampling_factor=np.float32(1.5761514)\n",
            "downsampling_factor=np.float32(1.5079256)\n",
            "downsampling_factor=np.float32(2.261888)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5593319)\n",
            "downsampling_factor=np.float32(1.5593319)\n",
            "downsampling_factor=np.float32(2.7778041)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4780371)\n",
            "downsampling_factor=np.float32(1.5348847)\n",
            "downsampling_factor=np.float32(3.4337819)\n",
            "downsampling_factor=np.float32(2.2891872)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4426761)\n",
            "downsampling_factor=np.float32(1.5628991)\n",
            "downsampling_factor=np.float32(1.6160876)\n",
            "downsampling_factor=np.float32(2.0650012)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5399284)\n",
            "downsampling_factor=np.float32(1.5969625)\n",
            "downsampling_factor=np.float32(2.2239635)\n",
            "downsampling_factor=np.float32(2.9356308)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.1494445)\n",
            "downsampling_factor=np.float32(1.2643889)\n",
            "downsampling_factor=np.float32(1.1683712)\n",
            "downsampling_factor=np.float32(1.2981898)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.6016988)\n",
            "downsampling_factor=np.float32(1.6016988)\n",
            "downsampling_factor=np.float32(2.8751895)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4818449)\n",
            "downsampling_factor=np.float32(1.4818449)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.623895)\n",
            "downsampling_factor=np.float32(1.8404146)\n",
            "downsampling_factor=np.float32(1.3563557)\n",
            "downsampling_factor=np.float32(1.8084742)\n",
            "downsampling_factor=np.float32(1.5235157)\n",
            "downsampling_factor=np.float32(1.5235157)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5391474)\n",
            "downsampling_factor=np.float32(1.5391474)\n",
            "downsampling_factor=np.float32(0.8557129)\n",
            "downsampling_factor=np.float32(2.5671372)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4741678)\n",
            "downsampling_factor=np.float32(1.5382624)\n",
            "downsampling_factor=np.float32(2.2044678)\n",
            "downsampling_factor=np.float32(2.543616)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.1699905)\n",
            "downsampling_factor=np.float32(1.8199869)\n",
            "downsampling_factor=np.float32(1.1641668)\n",
            "downsampling_factor=np.float32(1.3970007)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.2991472)\n",
            "downsampling_factor=np.float32(1.5589777)\n",
            "downsampling_factor=np.float32(0.91460675)\n",
            "downsampling_factor=np.float32(0.91460675)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.5624104)\n",
            "downsampling_factor=np.float32(1.5624104)\n",
            "downsampling_factor=np.float32(1.6491457)\n",
            "downsampling_factor=np.float32(2.3435237)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.4527435)\n",
            "downsampling_factor=np.float32(1.549593)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "downsampling_factor=np.float32(1.44765)\n",
            "downsampling_factor=np.float32(1.5055571)\n",
            "/content/hamer/hamer/demo.py:152: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  white_img = (torch.ones_like(batch['img'][n]).cpu() - DEFAULT_MEAN[:,None,None]/255) / (DEFAULT_STD[:,None,None]/255)\n",
            "/content/hamer/hamer/demo.py:153: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  input_patch = batch['img'][n].cpu() * (DEFAULT_STD[:,None,None]/255) + (DEFAULT_MEAN[:,None,None]/255)\n",
            "\n",
            "üéâ Processing complete! Results saved to: /content/data/processed/hamer_output\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "repo_path = \"/content/hamer/hamer\"\n",
        "input_frames = \"/content/ego_dex_data/rgb\"\n",
        "output_folder = \"/content/data/processed/hamer_output\"\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    print(f\"Could not find HaMeR at {repo_path}\")\n",
        "    print(\"If you cloned it inside another folder (like /content/hamer/hamer), please adjust 'repo_path' above.\")\n",
        "else:\n",
        "    print(f\"Found HaMeR repo. Starting processing on: {input_frames}\")\n",
        "\n",
        "    !cd {repo_path} && python demo.py \\\n",
        "        --img_folder {input_frames} \\\n",
        "        --out_folder {output_folder} \\\n",
        "        --batch_size=4 \\\n",
        "        --side_view \\\n",
        "        --save_mesh \\\n",
        "        --full_frame\n",
        "\n",
        "    print(f\"\\nProcessing complete! Results saved to: {output_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2ZEe2A2OPRyZ",
        "outputId": "9df91e7d-e5c1-4e51-a154-d005c12faa9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Zipping '/content/data/processed/hamer_output'...\n",
            "‚¨áÔ∏è Starting download...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b5a49b8f-32b0-494a-a80d-77c35c9f8536\", \"hamer_results_2.zip\", 68967341)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "folder_to_download = \"/content/data/processed/hamer_output\"\n",
        "zip_filename = \"/content/hamer_results_2.zip\"\n",
        "\n",
        "print(f\"Zipping '{folder_to_download}'...\")\n",
        "shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', folder_to_download)\n",
        "\n",
        "# 2. Download the zip\n",
        "print(\"Starting download...\")\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjSKCgsoUA7A"
      },
      "source": [
        "# **Depth Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4TBt6LQZEuy",
        "outputId": "92b7b210-3639-4af6-b71b-8783281a5114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Depth-Anything-V2'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 142 (delta 29), reused 26 (delta 26), pack-reused 84 (from 1)\u001b[K\n",
            "Receiving objects: 100% (142/142), 45.17 MiB | 26.09 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/DepthAnything/Depth-Anything-V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Law6Ilefc3mZ",
        "outputId": "dd610813-bcd5-4ac7-efd3-f44360ea6fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Depth-Anything-V2\n",
            "Collecting gradio_imageslider (from -r requirements.txt (line 1))\n",
            "  Downloading gradio_imageslider-0.0.20-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gradio==4.29.0 (from -r requirements.txt (line 2))\n",
            "  Downloading gradio-4.29.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.13.0.90)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.24.0+cpu)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.29.0->-r requirements.txt (line 2))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (1.0.0)\n",
            "Collecting gradio-client==0.16.1 (from gradio==4.29.0->-r requirements.txt (line 2))\n",
            "  Downloading gradio_client-0.16.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (1.3.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.29.0->-r requirements.txt (line 2))\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting numpy~=1.0 (from gradio==4.29.0->-r requirements.txt (line 2))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (3.11.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (26.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.2.2)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==4.29.0->-r requirements.txt (line 2))\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.0.22)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.14.14)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.29.0->-r requirements.txt (line 2))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==0.16.1->gradio==4.29.0->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.1->gradio==4.29.0->-r requirements.txt (line 2))\n",
            "  Downloading websockets-11.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python (from -r requirements.txt (line 4))\n",
            "  Downloading opencv_python-4.13.0.92-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 5)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 5)) (3.6.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (2.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.3->gradio==4.29.0->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.3->gradio==4.29.0->-r requirements.txt (line 2)) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.3->gradio==4.29.0->-r requirements.txt (line 2)) (4.67.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.3->gradio==4.29.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==4.29.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==4.29.0->-r requirements.txt (line 2)) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.29.0->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.29.0->-r requirements.txt (line 2)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.29.0->-r requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (13.9.4)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.29.0->-r requirements.txt (line 2)) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.29.0->-r requirements.txt (line 2)) (0.0.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (0.30.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (0.1.2)\n",
            "Downloading gradio-4.29.0-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.16.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m314.6/314.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading gradio_imageslider-0.0.20-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, tomlkit, pillow, numpy, markupsafe, aiofiles, opencv-python, gradio-client, gradio, gradio_imageslider\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.3\n",
            "    Uninstalling tomlkit-0.13.3:\n",
            "      Successfully uninstalled tomlkit-0.13.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.13.0.90\n",
            "    Uninstalling opencv-python-4.13.0.90:\n",
            "      Successfully uninstalled opencv-python-4.13.0.90\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.23.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-genai 1.61.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "dataproc-spark-connect 1.0.1 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 gradio-4.29.0 gradio-client-0.16.1 gradio_imageslider-0.0.20 markupsafe-2.1.5 numpy-1.26.4 opencv-python-4.11.0.86 pillow-10.4.0 tomlkit-0.12.0 websockets-11.0.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "229e88da200c407c9b11eb88fcc25bfa",
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%cd Depth-Anything-V2\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZiGdajGdTtG",
        "outputId": "595bdf57-cbf8-4838-a26d-a93fa1b0a5bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'Depth-Anything-V2'\n",
            "/content/Depth-Anything-V2\n",
            "xFormers not available\n",
            "xFormers not available\n",
            "Progress 1/1: /content/add_remove_lid/0.mp4\n"
          ]
        }
      ],
      "source": [
        "%cd Depth-Anything-V2\n",
        "!python run_video.py \\\n",
        "  --encoder vitl \\\n",
        "  --video-path \"/content/add_remove_lid/0.mp4\" --outdir video_depth_vis"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
